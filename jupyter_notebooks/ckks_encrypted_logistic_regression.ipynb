{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jAyGadKzRDs"
      },
      "source": [
        "Install TenSEAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMHycFfBzPoQ",
        "outputId": "23dcda59-ec9d-4ba6-af16-438529c0b5c5"
      },
      "source": [
        "pip install tenseal==0.3.0a1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal==0.3.0a1\n",
            "  Downloading tenseal-0.3.0a1-cp37-cp37m-manylinux2010_x86_64.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.0a1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_GaYB7lM2XF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda39179-5cc9-44b7-ea10-8a324cdda149"
      },
      "source": [
        "pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQjVhqAZ-mSB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from time import time\n",
        "# from sklearn.datasets.samples_generator import make_blobs\n",
        "import tenseal as ts\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "random.seed(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MTSFIQ_5EluN",
        "outputId": "b1f80e29-5650-45c3-9ebd-494a933b4391"
      },
      "source": [
        "def gen_random_data():\n",
        "  centers = [(-2, 2), (2, -2)]\n",
        "  cluster_std = [1, 1]\n",
        "  return make_blobs(n_samples=100, cluster_std=cluster_std, centers=centers, n_features=2, random_state=1)\n",
        "\n",
        "X, y = gen_random_data()\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f2d0a31eb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHrwHEujMWgo"
      },
      "source": [
        "def split_train_test(x, y, test_ratio=0.3):\n",
        "    idxs = [i for i in range(len(x))]\n",
        "    random.shuffle(idxs)\n",
        "    # delimiter between test and train data\n",
        "    delim = int(len(x) * test_ratio)\n",
        "    test_idxs, train_idxs = idxs[:delim], idxs[delim:]\n",
        "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMliCzgCHUi8"
      },
      "source": [
        "X_train, y_train, X_test, y_test = split_train_test(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "On2rg6bWPeFk",
        "outputId": "6d02b98f-9b12-4b98-88f1-f349965389c0"
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)\n",
        "w = clf.coef_[0]\n",
        "a = -w[0] / w[1]\n",
        "xx = np.linspace(-5, 5)\n",
        "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
        "print(clf.coef_, clf.intercept_)\n",
        "plt.plot(xx, yy, 'k-')\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.29568621 -2.22233134]] [0.18997685]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i655FIP7zd6E"
      },
      "source": [
        "**Encrypted LR Class**\n",
        "\n",
        "#### Parameters Update for LR\n",
        "\n",
        "For updating the parameter, the usual rule is as follows, where $x^{(i)}$ is the i'th input data\n",
        "\n",
        "$$\\theta_j = \\theta_j - \\alpha \\; [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + \\frac{\\lambda}{m} \\theta_j]$$\n",
        "\n",
        "However, due to homomorphic encryption constraint, we preferred to set $\\frac{\\lambda}{m} = 0.05$ which gets us to the following update rule\n",
        "\n",
        "$$\\theta_j = \\theta_j - \\alpha \\; [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + 0.05 \\theta_j]$$\n",
        "\n",
        "#### Sigmoid Approximation\n",
        "\n",
        "Since we can't simply compute sigmoid on encrypted data, we need to approximate it using a low degree polynomial, the lowest degree the better, as we aim to perform as little multiplications as possible, to be able to use smaller parameters and thus optimize computation. This tutorial uses a degree 3 polynomial from https://eprint.iacr.org/2018/462.pdf which approximates the sigmoid function in the range $[-5,5]$\n",
        "\n",
        "$$\\sigma(x) = 0.5 + 0.197 x - 0.004 x^3$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmhkKPJMOQHB"
      },
      "source": [
        "# parameters\n",
        "poly_mod_degree = 16384\n",
        "coeff_mod_bit_sizes = [37, 28, 28, 28, 28, 28, 28, 28, 37]\n",
        "# create TenSEALContext\n",
        "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
        "# global_scale is used for encoding stage\n",
        "ctx_training.global_scale = 2 ** 28\n",
        "ctx_training.generate_galois_keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD8hqkJYP4iO"
      },
      "source": [
        "def encrypt_train_data(X_train, y_train):\n",
        "  enc_x_train = [ts.ckks_vector(ctx_training, X_i) for X_i in X_train]\n",
        "  enc_y_train = [ts.ckks_vector(ctx_training, [y_i]) for y_i in y_train]\n",
        "  return enc_x_train, enc_y_train\n",
        "\n",
        "\n",
        "def heart_disease_data():\n",
        "    data = pd.read_csv(\"./framingham.csv\")\n",
        "    # drop rows with missing values\n",
        "    data = data.dropna()\n",
        "    # drop some features\n",
        "    data = data.drop(columns=[\"education\", \"currentSmoker\", \"BPMeds\", \"diabetes\", \"diaBP\", \"BMI\"])\n",
        "    # balance data\n",
        "    grouped = data.groupby('TenYearCHD')\n",
        "    data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=73).reset_index(drop=True))\n",
        "    # extract labels\n",
        "    y = data[\"TenYearCHD\"].values\n",
        "    data = data.drop(\"TenYearCHD\", 'columns')\n",
        "    # standardize data\n",
        "    data = (data - data.mean()) / data.std()\n",
        "    return data.values, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dzs9RQ3fE2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be711c57-f4b2-41ec-e1ae-063d542b4c08"
      },
      "source": [
        "X, y = heart_disease_data()\n",
        "X_train, y_train, X_test, y_test = split_train_test(X, y)\n",
        "t_start = time()\n",
        "enc_X_train, enc_y_train = encrypt_train_data(X_train, y_train)\n",
        "print(f\"Encryption runtime is {time() - t_start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encryption runtime is 57.80588912963867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsHEJz6LPsNd",
        "outputId": "61511bcd-19df-4209-c4c9-25b30e24dbef"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpWrHpbahZxq"
      },
      "source": [
        "def approximate_sigmoid(enc_x):\n",
        "        # We use the polynomial approximation of degree 3\n",
        "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "        # from https://eprint.iacr.org/2018/462.pdf\n",
        "        # which fits the function pretty well in the range [-5,5]\n",
        "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
        "\n",
        "def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_YsjA-cM3uO"
      },
      "source": [
        "With a scale of around 20-bits, we need 6 coefficients modulus with the same bit-size as the scale, plus the last coeffcient, which needs more bits, we are already out of the 4096 polynomial modulus degree (which requires < 109 total bit count of the coefficients modulus, if we consider 128-bit security), so we will use 8192. This will allow us to batch up to 4096 values in a single ciphertext, but we are far away from this limitation, so we shouldn't even think about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a9T_f7Rzf2n"
      },
      "source": [
        "class EncryptedLR:\n",
        "    def __init__(self, context, n_features, epoch, lr=0.01):\n",
        "      self.context = context\n",
        "      self.weight = np.zeros(n_features).tolist()\n",
        "      self.bias = [0]\n",
        "      # we accumulate gradients and counts the number of iterations\n",
        "      self._delta_w = 0\n",
        "      self._delta_b = 0\n",
        "      self._count = 0\n",
        "      self._lr = lr\n",
        "      self._epoch = epoch\n",
        "\n",
        "    def __forward(self, enc_x):\n",
        "        # this need to do the shift\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias # self.weight has wrong size\n",
        "        # this need to do the shift\n",
        "        enc_out = approximate_sigmoid(enc_out)\n",
        "        return enc_out\n",
        "\n",
        "    def __backward(self, enc_x, enc_out, enc_y):\n",
        "        out_minus_y = (enc_out - enc_y)\n",
        "        a = enc_x * out_minus_y\n",
        "        self._delta_w += a\n",
        "        self._delta_b += out_minus_y\n",
        "        self._count += 1\n",
        "\n",
        "    def __update_parameters(self):\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._lr * (self._delta_w * (1 / self._count))\n",
        "        self.bias -= self._lr * (self._delta_b * (1 / self._count))\n",
        "        # reset gradient accumulators and iterations count\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def train(self, enc_x_train, enc_y_train, X_train, y_train, X_test, y_test):\n",
        "        for epoch in range(self._epoch):\n",
        "          print(f\"---------------START epoch #{epoch + 1}---------------\")\n",
        "          self.__encrypt()\n",
        "          t_start = time()\n",
        "          for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
        "              enc_out = self.__forward(enc_x)\n",
        "              self.__backward(enc_x, enc_out, enc_y)\n",
        "          self.__update_parameters()\n",
        "          self.__decrypt()\n",
        "          t_end = time()\n",
        "          acc = self.__get_test_accuracy(X_test, y_test)\n",
        "          print(f\"Accuracy on test set at epoch #{epoch + 1} is {acc}\")\n",
        "          print(f\"Loss at epoch #{epoch + 1} is {self.__get_loss(X_train, y_train)}\")\n",
        "          print(f\"Time at epoch #{epoch + 1} is {t_end - t_start}\")\n",
        "          print(f\"---------------FINISH epoch #{epoch + 1}---------------\")\n",
        "\n",
        "    def __encrypt(self):\n",
        "        self.weight = ts.ckks_vector(self.context, self.weight)\n",
        "        self.bias = ts.ckks_vector(self.context, self.bias)\n",
        "\n",
        "    def __decrypt(self):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "    def __get_test_accuracy(self, X_test, y_test):\n",
        "        print(\"weights\", self.weight)\n",
        "        print(\"bias\", self.bias)\n",
        "        clf_acc = LogisticRegression()\n",
        "        clf_acc.classes_ = np.unique(y_test)\n",
        "        clf_acc.coef_ = np.array([self.weight])\n",
        "        clf_acc.intercept_ = np.array([self.bias])\n",
        "        #w = clf_acc.coef_[0]\n",
        "        #a = -w[0] / w[1]\n",
        "        #xx = np.linspace(-5, 5)\n",
        "        #yy = a * xx - (clf_acc.intercept_[0]) / w[1]\n",
        "\n",
        "        #plt.plot(xx, yy, 'k-')\n",
        "        #plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
        "        #plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")\n",
        "        #plt.show()\n",
        "        return clf_acc.score(X_test, y_test)\n",
        "\n",
        "    def __get_loss(self, X_train, y_train):\n",
        "      out = X_train.dot(np.array(self.weight)) + self.bias\n",
        "      out = sigmoid(out)\n",
        "      return (-y_train * np.log(out) - (1 - y_train) * np.log(1 - out)).mean()\n",
        "\n",
        "    def get_enc_coef(self):\n",
        "      return self.weight, self.bias\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y-tL3EkDQU-u",
        "outputId": "370dbb0a-572a-4d5a-f0df-4f49888793ba"
      },
      "source": [
        "eelr = EncryptedLR(context=ctx_training, n_features=9, epoch=100, lr=0.01)\n",
        "eelr.train(enc_X_train, enc_y_train, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------START epoch #1---------------\n",
            "weights [0.0007020939674935655, 0.0015579901808392593, 0.0003347598015338586, 0.00044716560179067867, 0.001224206290161402, 0.0007896788532518462, 0.0014608845780728697, 0.00028629526225894535, 0.000688043665617393]\n",
            "bias [-3.167596277335486e-05]\n",
            "Accuracy on test set at epoch #1 is 0.6826347305389222\n",
            "Loss at epoch #1 is 0.6923552315078962\n",
            "Time at epoch #1 is 383.89735102653503\n",
            "---------------FINISH epoch #1---------------\n",
            "---------------START epoch #2---------------\n",
            "weights [0.0013867007938673788, 0.0031070983597819672, 0.0006729139956739927, 0.0008829757634299943, 0.0024575524694765314, 0.0015448755637151512, 0.0028721955460317174, 0.0005589161469855741, 0.0013697448796040168]\n",
            "bias [-8.65935417125739e-05]\n",
            "Accuracy on test set at epoch #2 is 0.6766467065868264\n",
            "Loss at epoch #2 is 0.6915793200536383\n",
            "Time at epoch #2 is 382.28174471855164\n",
            "---------------FINISH epoch #2---------------\n",
            "---------------START epoch #3---------------\n",
            "weights [0.002075599488865748, 0.0046530115404720285, 0.0010096807332193964, 0.0013354138774309566, 0.0036923963546160385, 0.002316152585765986, 0.004311179366434485, 0.0008596551225717601, 0.0020586053012838176]\n",
            "bias [-0.00013398776210042483]\n",
            "Accuracy on test set at epoch #3 is 0.6796407185628742\n",
            "Loss at epoch #3 is 0.6908001509971345\n",
            "Time at epoch #3 is 381.92227268218994\n",
            "---------------FINISH epoch #3---------------\n",
            "---------------START epoch #4---------------\n",
            "weights [0.0027628531598429757, 0.006198844458998812, 0.0013478661061562204, 0.0017721466384358771, 0.00489962875573269, 0.0031183564229465303, 0.005709916093652298, 0.001142378149857601, 0.0027237137707016544]\n",
            "bias [-0.00017166419170211724]\n",
            "Accuracy on test set at epoch #4 is 0.6826347305389222\n",
            "Loss at epoch #4 is 0.6900342302766275\n",
            "Time at epoch #4 is 381.60944747924805\n",
            "---------------FINISH epoch #4---------------\n",
            "---------------START epoch #5---------------\n",
            "weights [0.003450709637501611, 0.007759026536464019, 0.0016807116053373277, 0.0021898623570917727, 0.006126985113453293, 0.0038826559409196417, 0.007106580289141638, 0.0014280454571258768, 0.0033585299459575405]\n",
            "bias [-0.00022487554683315247]\n",
            "Accuracy on test set at epoch #5 is 0.6796407185628742\n",
            "Loss at epoch #5 is 0.689273301178423\n",
            "Time at epoch #5 is 381.83962178230286\n",
            "---------------FINISH epoch #5---------------\n",
            "---------------START epoch #6---------------\n",
            "weights [0.004132450668326326, 0.009283387668130604, 0.002016904385822773, 0.0026169729400275545, 0.0073352052830138285, 0.0046180894259824495, 0.00850818603812964, 0.0016992281383478563, 0.004001640628904294]\n",
            "bias [-0.00025503271747062987]\n",
            "Accuracy on test set at epoch #6 is 0.6766467065868264\n",
            "Loss at epoch #6 is 0.688525078696416\n",
            "Time at epoch #6 is 381.49613666534424\n",
            "---------------FINISH epoch #6---------------\n",
            "---------------START epoch #7---------------\n",
            "weights [0.004816415370732144, 0.010799377642965997, 0.002356942827493306, 0.003052581160885093, 0.008532907971397212, 0.005424364163129978, 0.00995948954407821, 0.001962444159850349, 0.0046430145883055345]\n",
            "bias [-0.0003013190273584399]\n",
            "Accuracy on test set at epoch #7 is 0.6796407185628742\n",
            "Loss at epoch #7 is 0.6877704079962723\n",
            "Time at epoch #7 is 380.44083166122437\n",
            "---------------FINISH epoch #7---------------\n",
            "---------------START epoch #8---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b816e07a4480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meelr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncryptedLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meelr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-c17b3b97e18a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, enc_x_train, enc_y_train, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0menc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m               \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__decrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c17b3b97e18a>\u001b[0m in \u001b[0;36m__backward\u001b[0;34m(self, enc_x, enc_out, enc_y)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout_minus_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_minus_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_w\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_b\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mout_minus_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tenseal/tensors/abstract_tensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"AbstractTensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__imul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"AbstractTensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tenseal/tensors/ckksvector.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"CKKSVector\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk0DfnEAWPXA"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class LRGD:\n",
        "    def __init__(self, n_features, epoch, lr=0.01):\n",
        "      self.weight = np.zeros(n_features)\n",
        "      self.bias = np.zeros(1)\n",
        "      # we accumulate gradients and counts the number of iterations\n",
        "      self._delta_w = 0\n",
        "      self._delta_b = 0\n",
        "      self._count = 0\n",
        "      self._lr = lr\n",
        "      self._epoch = epoch\n",
        "\n",
        "    def __forward(self, x):\n",
        "        # this need to do the shift\n",
        "        out = x.dot(self.weight) + self.bias\n",
        "        # this need to do the shift\n",
        "        out = self.approximate_sigmoid(out)\n",
        "        return out\n",
        "\n",
        "    def __backward(self, x, out, y):\n",
        "        out_minus_y = (out - y)\n",
        "        a = x * out_minus_y\n",
        "        self._delta_w += a\n",
        "        self._delta_b += out_minus_y\n",
        "        self._count += 1\n",
        "\n",
        "    def __update_parameters(self):\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._lr * (self._delta_w * (1 / self._count))\n",
        "        self.bias -= self._lr * (self._delta_b * (1 / self._count))\n",
        "        # reset gradient accumulators and iterations count\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test):\n",
        "        for epoch in range(self._epoch):\n",
        "          print(f\"---------------START epoch #{epoch + 1}---------------\")\n",
        "          t_start = time()\n",
        "          for x, y in zip(X_train, y_train):\n",
        "              out = self.__forward(x)\n",
        "              self.__backward(x, out, y)\n",
        "          self.__update_parameters()\n",
        "          t_end = time()\n",
        "          acc = self.__get_test_accuracy(X_test, y_test)\n",
        "          print(f\"Accuracy on test set at epoch #{epoch + 1} is {acc}\")\n",
        "          print(f\"Loss at epoch #{epoch + 1} is {self.__get_loss(X_train, y_train)}\")\n",
        "          print(f\"Time at epoch #{epoch + 1} is {t_end - t_start}\")\n",
        "          print(f\"---------------FINISH epoch #{epoch + 1}---------------\")\n",
        "\n",
        "    def __get_test_accuracy(self, X_test, y_test):\n",
        "        print(\"weights\", self.weight)\n",
        "        print(\"bias\", self.bias)\n",
        "        clf_acc = LogisticRegression()\n",
        "        clf_acc.classes_ = np.unique(y_test)\n",
        "        clf_acc.coef_ = np.array([self.weight])\n",
        "        clf_acc.intercept_ = np.array([self.bias])\n",
        "        #w = clf_acc.coef_[0]\n",
        "        #a = -w[0] / w[1]\n",
        "        #xx = np.linspace(-5, 5)\n",
        "        #yy = a * xx - (clf_acc.intercept_[0]) / w[1]\n",
        "\n",
        "        #plt.plot(xx, yy, 'k-')\n",
        "        #plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
        "        #plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")\n",
        "        #plt.show()\n",
        "        return clf_acc.score(X_test, y_test)\n",
        "\n",
        "    def get_enc_coef(self):\n",
        "      return self.weight, self.bias\n",
        "\n",
        "    def __get_loss(self, X_train, y_train):\n",
        "      out = X_train.dot(np.array(self.weight)) + self.bias\n",
        "      out = sigmoid(out)\n",
        "      return (-y_train * np.log(out) - (1 - y_train) * np.log(1 - out)).mean()\n",
        "\n",
        "    def approximate_sigmoid(self, x):\n",
        "        # We use the polynomial approximation of degree 3\n",
        "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "        # from https://eprint.iacr.org/2018/462.pdf\n",
        "        # which fits the function pretty well in the range [-5,5]\n",
        "        return 0.5 + 0.197 * x - 0.004 * x * x * x\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6srKWAlMc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33f9d7-53b9-4d3e-9633-33175e95fdfd"
      },
      "source": [
        "lrgd = LRGD(n_features=2, epoch=100, lr=0.01)\n",
        "lrgd.train(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------START epoch #1---------------\n",
            "weights [ 0.00981938 -0.00997551]\n",
            "bias [0.00015714]\n",
            "Accuracy on test set at epoch #1 is 1.0\n",
            "Loss at epoch #1 is 0.6737700499764444\n",
            "Time at epoch #1 is 0.028554677963256836\n",
            "---------------FINISH epoch #1---------------\n",
            "---------------START epoch #2---------------\n",
            "weights [ 0.01946572 -0.01977627]\n",
            "bias [0.00031113]\n",
            "Accuracy on test set at epoch #2 is 1.0\n",
            "Loss at epoch #2 is 0.6551585459581651\n",
            "Time at epoch #2 is 0.03011608123779297\n",
            "---------------FINISH epoch #2---------------\n",
            "---------------START epoch #3---------------\n",
            "weights [ 0.02894211 -0.02940541]\n",
            "bias [0.00046202]\n",
            "Accuracy on test set at epoch #3 is 1.0\n",
            "Loss at epoch #3 is 0.6372835539081377\n",
            "Time at epoch #3 is 0.02848029136657715\n",
            "---------------FINISH epoch #3---------------\n",
            "---------------START epoch #4---------------\n",
            "weights [ 0.03825163 -0.03886604]\n",
            "bias [0.00060987]\n",
            "Accuracy on test set at epoch #4 is 1.0\n",
            "Loss at epoch #4 is 0.6201165092089621\n",
            "Time at epoch #4 is 0.02326798439025879\n",
            "---------------FINISH epoch #4---------------\n",
            "---------------START epoch #5---------------\n",
            "weights [ 0.04739735 -0.04816126]\n",
            "bias [0.00075474]\n",
            "Accuracy on test set at epoch #5 is 1.0\n",
            "Loss at epoch #5 is 0.6036294621767749\n",
            "Time at epoch #5 is 0.03151369094848633\n",
            "---------------FINISH epoch #5---------------\n",
            "---------------START epoch #6---------------\n",
            "weights [ 0.05638234 -0.05729415]\n",
            "bias [0.00089668]\n",
            "Accuracy on test set at epoch #6 is 1.0\n",
            "Loss at epoch #6 is 0.5877951332425796\n",
            "Time at epoch #6 is 0.029317140579223633\n",
            "---------------FINISH epoch #6---------------\n",
            "---------------START epoch #7---------------\n",
            "weights [ 0.06520962 -0.06626779]\n",
            "bias [0.00103575]\n",
            "Accuracy on test set at epoch #7 is 1.0\n",
            "Loss at epoch #7 is 0.572586958779513\n",
            "Time at epoch #7 is 0.025694608688354492\n",
            "---------------FINISH epoch #7---------------\n",
            "---------------START epoch #8---------------\n",
            "weights [ 0.0738822 -0.0750852]\n",
            "bias [0.00117201]\n",
            "Accuracy on test set at epoch #8 is 1.0\n",
            "Loss at epoch #8 is 0.5579791280882151\n",
            "Time at epoch #8 is 0.029462099075317383\n",
            "---------------FINISH epoch #8---------------\n",
            "---------------START epoch #9---------------\n",
            "weights [ 0.08240308 -0.0837494 ]\n",
            "bias [0.00130551]\n",
            "Accuracy on test set at epoch #9 is 1.0\n",
            "Loss at epoch #9 is 0.543946612123651\n",
            "Time at epoch #9 is 0.029471635818481445\n",
            "---------------FINISH epoch #9---------------\n",
            "---------------START epoch #10---------------\n",
            "weights [ 0.09077519 -0.09226338]\n",
            "bias [0.00143631]\n",
            "Accuracy on test set at epoch #10 is 1.0\n",
            "Loss at epoch #10 is 0.5304651845964766\n",
            "Time at epoch #10 is 0.031249523162841797\n",
            "---------------FINISH epoch #10---------------\n",
            "---------------START epoch #11---------------\n",
            "weights [ 0.09900146 -0.10063006]\n",
            "bias [0.00156447]\n",
            "Accuracy on test set at epoch #11 is 1.0\n",
            "Loss at epoch #11 is 0.5175114361119821\n",
            "Time at epoch #11 is 0.03248143196105957\n",
            "---------------FINISH epoch #11---------------\n",
            "---------------START epoch #12---------------\n",
            "weights [ 0.10708478 -0.10885238]\n",
            "bias [0.00169003]\n",
            "Accuracy on test set at epoch #12 is 1.0\n",
            "Loss at epoch #12 is 0.5050627820219812\n",
            "Time at epoch #12 is 0.0228116512298584\n",
            "---------------FINISH epoch #12---------------\n",
            "---------------START epoch #13---------------\n",
            "weights [ 0.11502797 -0.11693319]\n",
            "bias [0.00181305]\n",
            "Accuracy on test set at epoch #13 is 1.0\n",
            "Loss at epoch #13 is 0.4930974646621815\n",
            "Time at epoch #13 is 0.030797481536865234\n",
            "---------------FINISH epoch #13---------------\n",
            "---------------START epoch #14---------------\n",
            "weights [ 0.12283386 -0.12487533]\n",
            "bias [0.00193358]\n",
            "Accuracy on test set at epoch #14 is 1.0\n",
            "Loss at epoch #14 is 0.48159455063216455\n",
            "Time at epoch #14 is 0.030135154724121094\n",
            "---------------FINISH epoch #14---------------\n",
            "---------------START epoch #15---------------\n",
            "weights [ 0.13050521 -0.1326816 ]\n",
            "bias [0.00205168]\n",
            "Accuracy on test set at epoch #15 is 1.0\n",
            "Loss at epoch #15 is 0.4705339237496503\n",
            "Time at epoch #15 is 0.03198122978210449\n",
            "---------------FINISH epoch #15---------------\n",
            "---------------START epoch #16---------------\n",
            "weights [ 0.13804473 -0.14035474]\n",
            "bias [0.00216739]\n",
            "Accuracy on test set at epoch #16 is 1.0\n",
            "Loss at epoch #16 is 0.45989627427764035\n",
            "Time at epoch #16 is 0.03531241416931152\n",
            "---------------FINISH epoch #16---------------\n",
            "---------------START epoch #17---------------\n",
            "weights [ 0.14545512 -0.14789746]\n",
            "bias [0.00228077]\n",
            "Accuracy on test set at epoch #17 is 1.0\n",
            "Loss at epoch #17 is 0.44966308498449264\n",
            "Time at epoch #17 is 0.03476309776306152\n",
            "---------------FINISH epoch #17---------------\n",
            "---------------START epoch #18---------------\n",
            "weights [ 0.152739   -0.15531243]\n",
            "bias [0.00239187]\n",
            "Accuracy on test set at epoch #18 is 1.0\n",
            "Loss at epoch #18 is 0.4398166145549156\n",
            "Time at epoch #18 is 0.03709816932678223\n",
            "---------------FINISH epoch #18---------------\n",
            "---------------START epoch #19---------------\n",
            "weights [ 0.15989898 -0.16260225]\n",
            "bias [0.00250073]\n",
            "Accuracy on test set at epoch #19 is 1.0\n",
            "Loss at epoch #19 is 0.43033987882591035\n",
            "Time at epoch #19 is 0.04716372489929199\n",
            "---------------FINISH epoch #19---------------\n",
            "---------------START epoch #20---------------\n",
            "weights [ 0.16693758 -0.16976951]\n",
            "bias [0.0026074]\n",
            "Accuracy on test set at epoch #20 is 1.0\n",
            "Loss at epoch #20 is 0.4212166302771955\n",
            "Time at epoch #20 is 0.029273271560668945\n",
            "---------------FINISH epoch #20---------------\n",
            "---------------START epoch #21---------------\n",
            "weights [ 0.17385733 -0.17681673]\n",
            "bias [0.00271194]\n",
            "Accuracy on test set at epoch #21 is 1.0\n",
            "Loss at epoch #21 is 0.4124313361617031\n",
            "Time at epoch #21 is 0.03709149360656738\n",
            "---------------FINISH epoch #21---------------\n",
            "---------------START epoch #22---------------\n",
            "weights [ 0.18066067 -0.18374639]\n",
            "bias [0.00281438]\n",
            "Accuracy on test set at epoch #22 is 1.0\n",
            "Loss at epoch #22 is 0.40396915561918256\n",
            "Time at epoch #22 is 0.04253196716308594\n",
            "---------------FINISH epoch #22---------------\n",
            "---------------START epoch #23---------------\n",
            "weights [ 0.18735002 -0.19056093]\n",
            "bias [0.00291477]\n",
            "Accuracy on test set at epoch #23 is 1.0\n",
            "Loss at epoch #23 is 0.3958159160754006\n",
            "Time at epoch #23 is 0.03408694267272949\n",
            "---------------FINISH epoch #23---------------\n",
            "---------------START epoch #24---------------\n",
            "weights [ 0.19392773 -0.19726272]\n",
            "bias [0.00301316]\n",
            "Accuracy on test set at epoch #24 is 1.0\n",
            "Loss at epoch #24 is 0.3879580891913325\n",
            "Time at epoch #24 is 0.030475616455078125\n",
            "---------------FINISH epoch #24---------------\n",
            "---------------START epoch #25---------------\n",
            "weights [ 0.20039613 -0.20385412]\n",
            "bias [0.00310959]\n",
            "Accuracy on test set at epoch #25 is 1.0\n",
            "Loss at epoch #25 is 0.3803827665913536\n",
            "Time at epoch #25 is 0.030454397201538086\n",
            "---------------FINISH epoch #25---------------\n",
            "---------------START epoch #26---------------\n",
            "weights [ 0.20675749 -0.21033742]\n",
            "bias [0.0032041]\n",
            "Accuracy on test set at epoch #26 is 1.0\n",
            "Loss at epoch #26 is 0.37307763556693657\n",
            "Time at epoch #26 is 0.030866384506225586\n",
            "---------------FINISH epoch #26---------------\n",
            "---------------START epoch #27---------------\n",
            "weights [ 0.21301403 -0.21671487]\n",
            "bias [0.00329674]\n",
            "Accuracy on test set at epoch #27 is 1.0\n",
            "Loss at epoch #27 is 0.3660309549227603\n",
            "Time at epoch #27 is 0.03346419334411621\n",
            "---------------FINISH epoch #27---------------\n",
            "---------------START epoch #28---------------\n",
            "weights [ 0.21916795 -0.22298867]\n",
            "bias [0.00338754]\n",
            "Accuracy on test set at epoch #28 is 1.0\n",
            "Loss at epoch #28 is 0.3592315311054234\n",
            "Time at epoch #28 is 0.03859138488769531\n",
            "---------------FINISH epoch #28---------------\n",
            "---------------START epoch #29---------------\n",
            "weights [ 0.22522137 -0.22916097]\n",
            "bias [0.00347655]\n",
            "Accuracy on test set at epoch #29 is 1.0\n",
            "Loss at epoch #29 is 0.3526686947310288\n",
            "Time at epoch #29 is 0.017277956008911133\n",
            "---------------FINISH epoch #29---------------\n",
            "---------------START epoch #30---------------\n",
            "weights [ 0.23117639 -0.23523391]\n",
            "bias [0.0035638]\n",
            "Accuracy on test set at epoch #30 is 1.0\n",
            "Loss at epoch #30 is 0.34633227760663615\n",
            "Time at epoch #30 is 0.029571533203125\n",
            "---------------FINISH epoch #30---------------\n",
            "---------------START epoch #31---------------\n",
            "weights [ 0.23703505 -0.24120954]\n",
            "bias [0.00364933]\n",
            "Accuracy on test set at epoch #31 is 1.0\n",
            "Loss at epoch #31 is 0.3402125903217978\n",
            "Time at epoch #31 is 0.019563674926757812\n",
            "---------------FINISH epoch #31---------------\n",
            "---------------START epoch #32---------------\n",
            "weights [ 0.24279937 -0.24708988]\n",
            "bias [0.00373319]\n",
            "Accuracy on test set at epoch #32 is 1.0\n",
            "Loss at epoch #32 is 0.33430040046992887\n",
            "Time at epoch #32 is 0.02895188331604004\n",
            "---------------FINISH epoch #32---------------\n",
            "---------------START epoch #33---------------\n",
            "weights [ 0.24847131 -0.25287693]\n",
            "bias [0.00381541]\n",
            "Accuracy on test set at epoch #33 is 1.0\n",
            "Loss at epoch #33 is 0.3285869115449194\n",
            "Time at epoch #33 is 0.03153038024902344\n",
            "---------------FINISH epoch #33---------------\n",
            "---------------START epoch #34---------------\n",
            "weights [ 0.25405278 -0.25857262]\n",
            "bias [0.00389602]\n",
            "Accuracy on test set at epoch #34 is 1.0\n",
            "Loss at epoch #34 is 0.32306374254598486\n",
            "Time at epoch #34 is 0.03013324737548828\n",
            "---------------FINISH epoch #34---------------\n",
            "---------------START epoch #35---------------\n",
            "weights [ 0.25954567 -0.26417885]\n",
            "bias [0.00397506]\n",
            "Accuracy on test set at epoch #35 is 1.0\n",
            "Loss at epoch #35 is 0.31772290831309175\n",
            "Time at epoch #35 is 0.03163647651672363\n",
            "---------------FINISH epoch #35---------------\n",
            "---------------START epoch #36---------------\n",
            "weights [ 0.26495181 -0.26969748]\n",
            "bias [0.00405256]\n",
            "Accuracy on test set at epoch #36 is 1.0\n",
            "Loss at epoch #36 is 0.3125568006062025\n",
            "Time at epoch #36 is 0.03058648109436035\n",
            "---------------FINISH epoch #36---------------\n",
            "---------------START epoch #37---------------\n",
            "weights [ 0.27027301 -0.27513032]\n",
            "bias [0.00412857]\n",
            "Accuracy on test set at epoch #37 is 1.0\n",
            "Loss at epoch #37 is 0.3075581699338824\n",
            "Time at epoch #37 is 0.030107736587524414\n",
            "---------------FINISH epoch #37---------------\n",
            "---------------START epoch #38---------------\n",
            "weights [ 0.27551101 -0.28047914]\n",
            "bias [0.0042031]\n",
            "Accuracy on test set at epoch #38 is 1.0\n",
            "Loss at epoch #38 is 0.30272010813035966\n",
            "Time at epoch #38 is 0.03103041648864746\n",
            "---------------FINISH epoch #38---------------\n",
            "---------------START epoch #39---------------\n",
            "weights [ 0.28066753 -0.28574569]\n",
            "bias [0.0042762]\n",
            "Accuracy on test set at epoch #39 is 1.0\n",
            "Loss at epoch #39 is 0.2980360316747588\n",
            "Time at epoch #39 is 0.024791955947875977\n",
            "---------------FINISH epoch #39---------------\n",
            "---------------START epoch #40---------------\n",
            "weights [ 0.28574426 -0.29093165]\n",
            "bias [0.00434789]\n",
            "Accuracy on test set at epoch #40 is 1.0\n",
            "Loss at epoch #40 is 0.2934996657418218\n",
            "Time at epoch #40 is 0.02946019172668457\n",
            "---------------FINISH epoch #40---------------\n",
            "---------------START epoch #41---------------\n",
            "weights [ 0.29074285 -0.29603869]\n",
            "bias [0.0044182]\n",
            "Accuracy on test set at epoch #41 is 1.0\n",
            "Loss at epoch #41 is 0.2891050289698546\n",
            "Time at epoch #41 is 0.029019832611083984\n",
            "---------------FINISH epoch #41---------------\n",
            "---------------START epoch #42---------------\n",
            "weights [ 0.29566488 -0.30106843]\n",
            "bias [0.00448717]\n",
            "Accuracy on test set at epoch #42 is 1.0\n",
            "Loss at epoch #42 is 0.28484641892878537\n",
            "Time at epoch #42 is 0.029134750366210938\n",
            "---------------FINISH epoch #42---------------\n",
            "---------------START epoch #43---------------\n",
            "weights [ 0.30051194 -0.30602246]\n",
            "bias [0.00455483]\n",
            "Accuracy on test set at epoch #43 is 1.0\n",
            "Loss at epoch #43 is 0.2807183982689995\n",
            "Time at epoch #43 is 0.03394722938537598\n",
            "---------------FINISH epoch #43---------------\n",
            "---------------START epoch #44---------------\n",
            "weights [ 0.30528556 -0.31090232]\n",
            "bias [0.0046212]\n",
            "Accuracy on test set at epoch #44 is 1.0\n",
            "Loss at epoch #44 is 0.27671578152992116\n",
            "Time at epoch #44 is 0.03507733345031738\n",
            "---------------FINISH epoch #44---------------\n",
            "---------------START epoch #45---------------\n",
            "weights [ 0.30998723 -0.31570953]\n",
            "bias [0.00468631]\n",
            "Accuracy on test set at epoch #45 is 1.0\n",
            "Loss at epoch #45 is 0.2728336225860852\n",
            "Time at epoch #45 is 0.03530740737915039\n",
            "---------------FINISH epoch #45---------------\n",
            "---------------START epoch #46---------------\n",
            "weights [ 0.31461842 -0.32044556]\n",
            "bias [0.00475019]\n",
            "Accuracy on test set at epoch #46 is 1.0\n",
            "Loss at epoch #46 is 0.2690672027076003\n",
            "Time at epoch #46 is 0.024901866912841797\n",
            "---------------FINISH epoch #46---------------\n",
            "---------------START epoch #47---------------\n",
            "weights [ 0.31918056 -0.32511187]\n",
            "bias [0.00481287]\n",
            "Accuracy on test set at epoch #47 is 1.0\n",
            "Loss at epoch #47 is 0.2654120192113896\n",
            "Time at epoch #47 is 0.030909061431884766\n",
            "---------------FINISH epoch #47---------------\n",
            "---------------START epoch #48---------------\n",
            "weights [ 0.32367506 -0.32970986]\n",
            "bias [0.00487436]\n",
            "Accuracy on test set at epoch #48 is 1.0\n",
            "Loss at epoch #48 is 0.26186377467935945\n",
            "Time at epoch #48 is 0.029447317123413086\n",
            "---------------FINISH epoch #48---------------\n",
            "---------------START epoch #49---------------\n",
            "weights [ 0.32810327 -0.33424092]\n",
            "bias [0.00493471]\n",
            "Accuracy on test set at epoch #49 is 1.0\n",
            "Loss at epoch #49 is 0.25841836671963386\n",
            "Time at epoch #49 is 0.034557342529296875\n",
            "---------------FINISH epoch #49---------------\n",
            "---------------START epoch #50---------------\n",
            "weights [ 0.33246654 -0.3387064 ]\n",
            "bias [0.00499392]\n",
            "Accuracy on test set at epoch #50 is 1.0\n",
            "Loss at epoch #50 is 0.25507187824717376\n",
            "Time at epoch #50 is 0.020466327667236328\n",
            "---------------FINISH epoch #50---------------\n",
            "---------------START epoch #51---------------\n",
            "weights [ 0.33676617 -0.3431076 ]\n",
            "bias [0.00505204]\n",
            "Accuracy on test set at epoch #51 is 1.0\n",
            "Loss at epoch #51 is 0.2518205682604313\n",
            "Time at epoch #51 is 0.03425765037536621\n",
            "---------------FINISH epoch #51---------------\n",
            "---------------START epoch #52---------------\n",
            "weights [ 0.34100343 -0.34744583]\n",
            "bias [0.00510907]\n",
            "Accuracy on test set at epoch #52 is 1.0\n",
            "Loss at epoch #52 is 0.2486608630911435\n",
            "Time at epoch #52 is 0.03359365463256836\n",
            "---------------FINISH epoch #52---------------\n",
            "---------------START epoch #53---------------\n",
            "weights [ 0.34517957 -0.35172234]\n",
            "bias [0.00516504]\n",
            "Accuracy on test set at epoch #53 is 1.0\n",
            "Loss at epoch #53 is 0.24558934810492486\n",
            "Time at epoch #53 is 0.029962778091430664\n",
            "---------------FINISH epoch #53---------------\n",
            "---------------START epoch #54---------------\n",
            "weights [ 0.3492958  -0.35593836]\n",
            "bias [0.00521998]\n",
            "Accuracy on test set at epoch #54 is 1.0\n",
            "Loss at epoch #54 is 0.24260275983094556\n",
            "Time at epoch #54 is 0.01670670509338379\n",
            "---------------FINISH epoch #54---------------\n",
            "---------------START epoch #55---------------\n",
            "weights [ 0.35335332 -0.36009509]\n",
            "bias [0.00527391]\n",
            "Accuracy on test set at epoch #55 is 1.0\n",
            "Loss at epoch #55 is 0.23969797849966815\n",
            "Time at epoch #55 is 0.029059410095214844\n",
            "---------------FINISH epoch #55---------------\n",
            "---------------START epoch #56---------------\n",
            "weights [ 0.35735329 -0.36419371]\n",
            "bias [0.00532685]\n",
            "Accuracy on test set at epoch #56 is 1.0\n",
            "Loss at epoch #56 is 0.23687202096834256\n",
            "Time at epoch #56 is 0.033127784729003906\n",
            "---------------FINISH epoch #56---------------\n",
            "---------------START epoch #57---------------\n",
            "weights [ 0.36129684 -0.36823535]\n",
            "bias [0.00537882]\n",
            "Accuracy on test set at epoch #57 is 1.0\n",
            "Loss at epoch #57 is 0.23412203401471263\n",
            "Time at epoch #57 is 0.020780324935913086\n",
            "---------------FINISH epoch #57---------------\n",
            "---------------START epoch #58---------------\n",
            "weights [ 0.36518508 -0.37222115]\n",
            "bias [0.00542984]\n",
            "Accuracy on test set at epoch #58 is 1.0\n",
            "Loss at epoch #58 is 0.23144528798015948\n",
            "Time at epoch #58 is 0.028521060943603516\n",
            "---------------FINISH epoch #58---------------\n",
            "---------------START epoch #59---------------\n",
            "weights [ 0.36901911 -0.3761522 ]\n",
            "bias [0.00547993]\n",
            "Accuracy on test set at epoch #59 is 1.0\n",
            "Loss at epoch #59 is 0.2288391707442842\n",
            "Time at epoch #59 is 0.03296232223510742\n",
            "---------------FINISH epoch #59---------------\n",
            "---------------START epoch #60---------------\n",
            "weights [ 0.37279996 -0.38002956]\n",
            "bias [0.00552911]\n",
            "Accuracy on test set at epoch #60 is 1.0\n",
            "Loss at epoch #60 is 0.22630118201371077\n",
            "Time at epoch #60 is 0.025557518005371094\n",
            "---------------FINISH epoch #60---------------\n",
            "---------------START epoch #61---------------\n",
            "weights [ 0.37652869 -0.38385428]\n",
            "bias [0.0055774]\n",
            "Accuracy on test set at epoch #61 is 1.0\n",
            "Loss at epoch #61 is 0.223828927908661\n",
            "Time at epoch #61 is 0.03105330467224121\n",
            "---------------FINISH epoch #61---------------\n",
            "---------------START epoch #62---------------\n",
            "weights [ 0.3802063  -0.38762739]\n",
            "bias [0.00562482]\n",
            "Accuracy on test set at epoch #62 is 1.0\n",
            "Loss at epoch #62 is 0.22142011583161267\n",
            "Time at epoch #62 is 0.032639265060424805\n",
            "---------------FINISH epoch #62---------------\n",
            "---------------START epoch #63---------------\n",
            "weights [ 0.38383378 -0.39134988]\n",
            "bias [0.00567139]\n",
            "Accuracy on test set at epoch #63 is 1.0\n",
            "Loss at epoch #63 is 0.21907254960309616\n",
            "Time at epoch #63 is 0.030444860458374023\n",
            "---------------FINISH epoch #63---------------\n",
            "---------------START epoch #64---------------\n",
            "weights [ 0.3874121  -0.39502273]\n",
            "bias [0.00571712]\n",
            "Accuracy on test set at epoch #64 is 1.0\n",
            "Loss at epoch #64 is 0.21678412485040874\n",
            "Time at epoch #64 is 0.02943706512451172\n",
            "---------------FINISH epoch #64---------------\n",
            "---------------START epoch #65---------------\n",
            "weights [ 0.3909422  -0.39864689]\n",
            "bias [0.00576203]\n",
            "Accuracy on test set at epoch #65 is 1.0\n",
            "Loss at epoch #65 is 0.21455282463573008\n",
            "Time at epoch #65 is 0.027989864349365234\n",
            "---------------FINISH epoch #65---------------\n",
            "---------------START epoch #66---------------\n",
            "weights [ 0.39442501 -0.40222329]\n",
            "bias [0.00580614]\n",
            "Accuracy on test set at epoch #66 is 1.0\n",
            "Loss at epoch #66 is 0.21237671531080296\n",
            "Time at epoch #66 is 0.03326010704040527\n",
            "---------------FINISH epoch #66---------------\n",
            "---------------START epoch #67---------------\n",
            "weights [ 0.39786141 -0.40575285]\n",
            "bias [0.00584947]\n",
            "Accuracy on test set at epoch #67 is 1.0\n",
            "Loss at epoch #67 is 0.2102539425859977\n",
            "Time at epoch #67 is 0.03403615951538086\n",
            "---------------FINISH epoch #67---------------\n",
            "---------------START epoch #68---------------\n",
            "weights [ 0.40125231 -0.40923645]\n",
            "bias [0.00589203]\n",
            "Accuracy on test set at epoch #68 is 1.0\n",
            "Loss at epoch #68 is 0.2081827278022113\n",
            "Time at epoch #68 is 0.03193378448486328\n",
            "---------------FINISH epoch #68---------------\n",
            "---------------START epoch #69---------------\n",
            "weights [ 0.40459855 -0.41267497]\n",
            "bias [0.00593383]\n",
            "Accuracy on test set at epoch #69 is 1.0\n",
            "Loss at epoch #69 is 0.20616136439465618\n",
            "Time at epoch #69 is 0.028224468231201172\n",
            "---------------FINISH epoch #69---------------\n",
            "---------------START epoch #70---------------\n",
            "weights [ 0.40790098 -0.41606924]\n",
            "bias [0.0059749]\n",
            "Accuracy on test set at epoch #70 is 1.0\n",
            "Loss at epoch #70 is 0.20418821453817335\n",
            "Time at epoch #70 is 0.029422760009765625\n",
            "---------------FINISH epoch #70---------------\n",
            "---------------START epoch #71---------------\n",
            "weights [ 0.41116042 -0.41942012]\n",
            "bias [0.00601525]\n",
            "Accuracy on test set at epoch #71 is 1.0\n",
            "Loss at epoch #71 is 0.20226170596425663\n",
            "Time at epoch #71 is 0.02850937843322754\n",
            "---------------FINISH epoch #71---------------\n",
            "---------------START epoch #72---------------\n",
            "weights [ 0.41437768 -0.4227284 ]\n",
            "bias [0.00605489]\n",
            "Accuracy on test set at epoch #72 is 1.0\n",
            "Loss at epoch #72 is 0.20038032894050506\n",
            "Time at epoch #72 is 0.02905440330505371\n",
            "---------------FINISH epoch #72---------------\n",
            "---------------START epoch #73---------------\n",
            "weights [ 0.41755354 -0.42599489]\n",
            "bias [0.00609384]\n",
            "Accuracy on test set at epoch #73 is 1.0\n",
            "Loss at epoch #73 is 0.1985426334037208\n",
            "Time at epoch #73 is 0.02923750877380371\n",
            "---------------FINISH epoch #73---------------\n",
            "---------------START epoch #74---------------\n",
            "weights [ 0.42068878 -0.42922035]\n",
            "bias [0.00613211]\n",
            "Accuracy on test set at epoch #74 is 1.0\n",
            "Loss at epoch #74 is 0.19674722623834992\n",
            "Time at epoch #74 is 0.028847217559814453\n",
            "---------------FINISH epoch #74---------------\n",
            "---------------START epoch #75---------------\n",
            "weights [ 0.42378414 -0.43240555]\n",
            "bias [0.00616971]\n",
            "Accuracy on test set at epoch #75 is 1.0\n",
            "Loss at epoch #75 is 0.19499276869241672\n",
            "Time at epoch #75 is 0.029202938079833984\n",
            "---------------FINISH epoch #75---------------\n",
            "---------------START epoch #76---------------\n",
            "weights [ 0.42684035 -0.43555124]\n",
            "bias [0.00620666]\n",
            "Accuracy on test set at epoch #76 is 1.0\n",
            "Loss at epoch #76 is 0.19327797392353538\n",
            "Time at epoch #76 is 0.029868125915527344\n",
            "---------------FINISH epoch #76---------------\n",
            "---------------START epoch #77---------------\n",
            "weights [ 0.42985815 -0.43865813]\n",
            "bias [0.00624297]\n",
            "Accuracy on test set at epoch #77 is 1.0\n",
            "Loss at epoch #77 is 0.19160160466798987\n",
            "Time at epoch #77 is 0.029430150985717773\n",
            "---------------FINISH epoch #77---------------\n",
            "---------------START epoch #78---------------\n",
            "weights [ 0.43283823 -0.44172693]\n",
            "bias [0.00627866]\n",
            "Accuracy on test set at epoch #78 is 1.0\n",
            "Loss at epoch #78 is 0.18996247102626196\n",
            "Time at epoch #78 is 0.033663272857666016\n",
            "---------------FINISH epoch #78---------------\n",
            "---------------START epoch #79---------------\n",
            "weights [ 0.43578127 -0.44475835]\n",
            "bias [0.00631373]\n",
            "Accuracy on test set at epoch #79 is 1.0\n",
            "Loss at epoch #79 is 0.18835942835875533\n",
            "Time at epoch #79 is 0.029590606689453125\n",
            "---------------FINISH epoch #79---------------\n",
            "---------------START epoch #80---------------\n",
            "weights [ 0.43868796 -0.44775306]\n",
            "bias [0.00634821]\n",
            "Accuracy on test set at epoch #80 is 1.0\n",
            "Loss at epoch #80 is 0.18679137528580766\n",
            "Time at epoch #80 is 0.028568029403686523\n",
            "---------------FINISH epoch #80---------------\n",
            "---------------START epoch #81---------------\n",
            "weights [ 0.44155894 -0.45071172]\n",
            "bias [0.00638209]\n",
            "Accuracy on test set at epoch #81 is 1.0\n",
            "Loss at epoch #81 is 0.18525725178641647\n",
            "Time at epoch #81 is 0.029418230056762695\n",
            "---------------FINISH epoch #81---------------\n",
            "---------------START epoch #82---------------\n",
            "weights [ 0.44439486 -0.45363499]\n",
            "bias [0.0064154]\n",
            "Accuracy on test set at epoch #82 is 1.0\n",
            "Loss at epoch #82 is 0.1837560373904096\n",
            "Time at epoch #82 is 0.028380870819091797\n",
            "---------------FINISH epoch #82---------------\n",
            "---------------START epoch #83---------------\n",
            "weights [ 0.44719636 -0.4565235 ]\n",
            "bias [0.00644814]\n",
            "Accuracy on test set at epoch #83 is 1.0\n",
            "Loss at epoch #83 is 0.1822867494590894\n",
            "Time at epoch #83 is 0.032948970794677734\n",
            "---------------FINISH epoch #83---------------\n",
            "---------------START epoch #84---------------\n",
            "weights [ 0.44996404 -0.45937786]\n",
            "bias [0.00648033]\n",
            "Accuracy on test set at epoch #84 is 1.0\n",
            "Loss at epoch #84 is 0.1808484415496532\n",
            "Time at epoch #84 is 0.030061721801757812\n",
            "---------------FINISH epoch #84---------------\n",
            "---------------START epoch #85---------------\n",
            "weights [ 0.45269851 -0.4621987 ]\n",
            "bias [0.00651197]\n",
            "Accuracy on test set at epoch #85 is 1.0\n",
            "Loss at epoch #85 is 0.17944020185895743\n",
            "Time at epoch #85 is 0.01615619659423828\n",
            "---------------FINISH epoch #85---------------\n",
            "---------------START epoch #86---------------\n",
            "weights [ 0.45540035 -0.4649866 ]\n",
            "bias [0.00654308]\n",
            "Accuracy on test set at epoch #86 is 1.0\n",
            "Loss at epoch #86 is 0.1780611517424375\n",
            "Time at epoch #86 is 0.0328831672668457\n",
            "---------------FINISH epoch #86---------------\n",
            "---------------START epoch #87---------------\n",
            "weights [ 0.45807015 -0.46774214]\n",
            "bias [0.00657367]\n",
            "Accuracy on test set at epoch #87 is 1.0\n",
            "Loss at epoch #87 is 0.17671044430423005\n",
            "Time at epoch #87 is 0.030637025833129883\n",
            "---------------FINISH epoch #87---------------\n",
            "---------------START epoch #88---------------\n",
            "weights [ 0.46070846 -0.4704659 ]\n",
            "bias [0.00660374]\n",
            "Accuracy on test set at epoch #88 is 1.0\n",
            "Loss at epoch #88 is 0.17538726305476368\n",
            "Time at epoch #88 is 0.027296781539916992\n",
            "---------------FINISH epoch #88---------------\n",
            "---------------START epoch #89---------------\n",
            "weights [ 0.46331585 -0.47315843]\n",
            "bias [0.00663331]\n",
            "Accuracy on test set at epoch #89 is 1.0\n",
            "Loss at epoch #89 is 0.1740908206322916\n",
            "Time at epoch #89 is 0.029418468475341797\n",
            "---------------FINISH epoch #89---------------\n",
            "---------------START epoch #90---------------\n",
            "weights [ 0.46589284 -0.47582029]\n",
            "bias [0.00666239]\n",
            "Accuracy on test set at epoch #90 is 1.0\n",
            "Loss at epoch #90 is 0.17282035758503586\n",
            "Time at epoch #90 is 0.015846729278564453\n",
            "---------------FINISH epoch #90---------------\n",
            "---------------START epoch #91---------------\n",
            "weights [ 0.46843997 -0.47845199]\n",
            "bias [0.00669099]\n",
            "Accuracy on test set at epoch #91 is 1.0\n",
            "Loss at epoch #91 is 0.17157514121079684\n",
            "Time at epoch #91 is 0.033245086669921875\n",
            "---------------FINISH epoch #91---------------\n",
            "---------------START epoch #92---------------\n",
            "weights [ 0.47095775 -0.48105407]\n",
            "bias [0.00671911]\n",
            "Accuracy on test set at epoch #92 is 1.0\n",
            "Loss at epoch #92 is 0.1703544644510558\n",
            "Time at epoch #92 is 0.03371024131774902\n",
            "---------------FINISH epoch #92---------------\n",
            "---------------START epoch #93---------------\n",
            "weights [ 0.47344669 -0.48362703]\n",
            "bias [0.00674676]\n",
            "Accuracy on test set at epoch #93 is 1.0\n",
            "Loss at epoch #93 is 0.16915764483676257\n",
            "Time at epoch #93 is 0.02651357650756836\n",
            "---------------FINISH epoch #93---------------\n",
            "---------------START epoch #94---------------\n",
            "weights [ 0.47590729 -0.48617138]\n",
            "bias [0.00677396]\n",
            "Accuracy on test set at epoch #94 is 1.0\n",
            "Loss at epoch #94 is 0.16798402348315528\n",
            "Time at epoch #94 is 0.03084254264831543\n",
            "---------------FINISH epoch #94---------------\n",
            "---------------START epoch #95---------------\n",
            "weights [ 0.47834004 -0.4886876 ]\n",
            "bias [0.00680072]\n",
            "Accuracy on test set at epoch #95 is 1.0\n",
            "Loss at epoch #95 is 0.1668329641311037\n",
            "Time at epoch #95 is 0.029103755950927734\n",
            "---------------FINISH epoch #95---------------\n",
            "---------------START epoch #96---------------\n",
            "weights [ 0.4807454  -0.49117619]\n",
            "bias [0.00682703]\n",
            "Accuracy on test set at epoch #96 is 1.0\n",
            "Loss at epoch #96 is 0.16570385223260733\n",
            "Time at epoch #96 is 0.02932453155517578\n",
            "---------------FINISH epoch #96---------------\n",
            "---------------START epoch #97---------------\n",
            "weights [ 0.48312384 -0.49363759]\n",
            "bias [0.00685291]\n",
            "Accuracy on test set at epoch #97 is 1.0\n",
            "Loss at epoch #97 is 0.1645960940782068\n",
            "Time at epoch #97 is 0.03012871742248535\n",
            "---------------FINISH epoch #97---------------\n",
            "---------------START epoch #98---------------\n",
            "weights [ 0.48547582 -0.49607229]\n",
            "bias [0.00687838]\n",
            "Accuracy on test set at epoch #98 is 1.0\n",
            "Loss at epoch #98 is 0.16350911596419104\n",
            "Time at epoch #98 is 0.028292417526245117\n",
            "---------------FINISH epoch #98---------------\n",
            "---------------START epoch #99---------------\n",
            "weights [ 0.48780178 -0.49848072]\n",
            "bias [0.00690342]\n",
            "Accuracy on test set at epoch #99 is 1.0\n",
            "Loss at epoch #99 is 0.1624423633975966\n",
            "Time at epoch #99 is 0.021970033645629883\n",
            "---------------FINISH epoch #99---------------\n",
            "---------------START epoch #100---------------\n",
            "weights [ 0.49010216 -0.50086332]\n",
            "bias [0.00692806]\n",
            "Accuracy on test set at epoch #100 is 1.0\n",
            "Loss at epoch #100 is 0.16139530033710545\n",
            "Time at epoch #100 is 0.02993321418762207\n",
            "---------------FINISH epoch #100---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxDdhpnJoAE8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def nparraytocsv(nparray, name):\n",
        "        name = name + \".csv\"\n",
        "        np.savetxt(name, nparray, delimiter=',', fmt='%f')\n",
        "\n",
        "def csvtonparray(name):\n",
        "        tmp = np.genfromtxt(name, delimiter=',')\n",
        "        if (np.isnan(tmp[0,0])):\n",
        "          X = tmp[1:,1:-1]\n",
        "          y = tmp[1:,-1]\n",
        "        else:\n",
        "          X = tmp[:,0:-1]\n",
        "          y = tmp[:,-1]\n",
        "        return X, y\n",
        "\n",
        "def concatnparrays(nparray1, nparray2):\n",
        "        return np.concatenate((nparray1, nparray2[:,None]),axis=1)\n",
        "\n",
        "\n",
        "# nparraytocsv(concatnparrays(X,y), \"dataset\")\n",
        "\n",
        "X, y = csvtonparray(\"LogReg_sample_dataset.csv\")\n",
        "# X, y = heart_disease_data()\n",
        "X_train, y_train, X_test, y_test = split_train_test(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7piXHi59oaJ",
        "outputId": "802385d0-64f6-4587-df68-a212f6528a3e"
      },
      "source": [
        "import psutil\n",
        "\n",
        "psutil.virtual_memory().used"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "632213504"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3kJXzm5ByU7"
      },
      "source": [
        "enc_X, enc_y = encrypt_train_data(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ruSFub3-7f"
      },
      "source": [
        "def csvtonparray_random_sampling(name, sample_number):\n",
        "        tmp = np.genfromtxt(name, delimiter=',')\n",
        "        if (np.isnan(tmp[0,0])):\n",
        "          tmp = tmp[1:, 1:]\n",
        "          np.random.shuffle(tmp)\n",
        "          #print(tmp[0])\n",
        "          #print('tmp size: ' , tmp.shape())\n",
        "          #ids = np.random.randint(50000, size = sample_number)\n",
        "          #tmp = tmp[np.random.choice(tmp.shape[0], 30000, replace=False), :]\n",
        "          #tmp = tmp[ids, :]\n",
        "          #print('tmp size after: ' , tmp[0].shape())\n",
        "          X = tmp[:sample_number,0:-1]\n",
        "          y = tmp[:sample_number,-1]\n",
        "        else:\n",
        "          np.random.shuffle(tmp)\n",
        "          #print(tmp[0])\n",
        "          #print('tmp size: ' , tmp.shape())\n",
        "          #ids = np.random.randint(50000, size = sample_number)\n",
        "          #tmp = tmp[np.random.choice(tmp.shape[0], 30000, replace=False), :]\n",
        "          #tmp = tmp[ids, :]\n",
        "          #print('tmp size after: ' , tmp[0].shape())\n",
        "          X = tmp[:sample_number,0:-1]\n",
        "          y = tmp[:sample_number,-1]\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U117_zL51Gvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd35aa9-3d17-4107-ed10-03a698a59867"
      },
      "source": [
        "HRF_x, HRF_y = csvtonparray_random_sampling(\"HRF_samples_big.csv\", 1000)\n",
        "print(HRF_x[0])\n",
        "print(HRF_y[0])\n",
        "print(HRF_x.shape)\n",
        "print(HRF_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.26270492 -3.23084439  2.22509517  0.03978098 -3.23084439]\n",
            "0.0\n",
            "(10000, 5)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEdJWlSXAspJ"
      },
      "source": [
        "HRF_x, HRF_y = csvtonparray(\"HRF_samples_big.csv\")\n",
        "print(HRF_x[0])\n",
        "print(HRF_y[0])\n",
        "print(HRF_x.shape)\n",
        "print(HRF_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0wongoxU_II"
      },
      "source": [
        "#code cell to upgrade the Colab to 25gb ram\n",
        "[1]*10**10\n",
        "#a = []\n",
        "#while(1):\n",
        "#    a.append('1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQBl884YTRr6"
      },
      "source": [
        "HRF_x, HRF_y = csvtonparray_random_sampling(\"HRF_samples_big.csv\", 2000)\n",
        "HRF_x_train, HRF_y_train, HRF_x_test, HRF_y_test = split_train_test(HRF_x, HRF_y)\n",
        "\n",
        "enc_HRF_x_train, enc_HRF_y_train = encrypt_train_data(HRF_x_train, HRF_y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoOZPBxMK4PU"
      },
      "source": [
        "HRF_x, HRF_y = csvtonparray(\"HRF_samples_big.csv\")\n",
        "HRF_x_train, HRF_y_train, HRF_x_test, HRF_y_test = split_train_test(HRF_x, HRF_y)\n",
        "\n",
        "enc_HRF_x_train, enc_HRF_y_train = encrypt_train_data(HRF_x_train, HRF_y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuIFbkMON9e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04267c2-39af-4741-cc2c-a24371b66ad8"
      },
      "source": [
        "eelr = EncryptedLR(context=ctx_training, n_features=len(HRF_x_train[0,:]), epoch=100, lr=1)\n",
        "eelr.train(enc_HRF_x_train, enc_HRF_y_train, HRF_x_train, HRF_y_train, HRF_x_test, HRF_y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------START epoch #1---------------\n",
            "weights [0.5096156753124839, -0.20609416918324758, 1.494099112684812, 2.1703677271992303, -0.22296554616006234]\n",
            "bias [0.008313191442107076]\n",
            "Accuracy on test set at epoch #1 is 0.5233333333333333\n",
            "Loss at epoch #1 is nan\n",
            "Time at epoch #1 is 591.2497644424438\n",
            "---------------FINISH epoch #1---------------\n",
            "---------------START epoch #2---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [239.6487993061462, -189.92292012434706, 652.1354251904318, 776.1235740730324, -189.9565537994645]\n",
            "bias [80.8565706206167]\n",
            "Accuracy on test set at epoch #2 is 0.5216666666666666\n",
            "Loss at epoch #2 is nan\n",
            "Time at epoch #2 is 588.6539278030396\n",
            "---------------FINISH epoch #2---------------\n",
            "---------------START epoch #3---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [12249.212959761555, 8410.482679904562, 29838.35608221865, -4665.728947361352, 6825.23435065629]\n",
            "bias [5734149544.462393]\n",
            "Accuracy on test set at epoch #3 is 0.5183333333333333\n",
            "Loss at epoch #3 is nan\n",
            "Time at epoch #3 is 588.9826507568359\n",
            "---------------FINISH epoch #3---------------\n",
            "---------------START epoch #4---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-12598.007426942797, -342.66773845270745, 3250.754275780304, 10933.764516918776, -9643.549354854187]\n",
            "bias [1.8863987639197454e+19]\n",
            "Accuracy on test set at epoch #4 is 0.5183333333333333\n",
            "Loss at epoch #4 is nan\n",
            "Time at epoch #4 is 589.7850499153137\n",
            "---------------FINISH epoch #4---------------\n",
            "---------------START epoch #5---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [18606.826922229262, 3537.5905177721306, -8689.74872925677, -5232.709565609763, -9802.06567560718]\n",
            "bias [-4.390944010576494e+19]\n",
            "Accuracy on test set at epoch #5 is 0.4816666666666667\n",
            "Loss at epoch #5 is nan\n",
            "Time at epoch #5 is 590.2010548114777\n",
            "---------------FINISH epoch #5---------------\n",
            "---------------START epoch #6---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-16349.514947683085, 3736.89179108655, -3032.641240705562, 3478.7267608963593, 998.1242243251863]\n",
            "bias [9.476726116773e+20]\n",
            "Accuracy on test set at epoch #6 is 0.5183333333333333\n",
            "Loss at epoch #6 is nan\n",
            "Time at epoch #6 is 590.9813990592957\n",
            "---------------FINISH epoch #6---------------\n",
            "---------------START epoch #7---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-544.5054681690926, -5790.259413072668, 25883.968548255012, 19549.518649938225, 11198.221700495917]\n",
            "bias [-1.1226176429494868e+20]\n",
            "Accuracy on test set at epoch #7 is 0.4816666666666667\n",
            "Loss at epoch #7 is nan\n",
            "Time at epoch #7 is 590.6868450641632\n",
            "---------------FINISH epoch #7---------------\n",
            "---------------START epoch #8---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [2523.396857233387, -13924.838134721518, -8528.489483776115, 9795.294626994802, 1847.2410415354188]\n",
            "bias [-4.681752563752966e+20]\n",
            "Accuracy on test set at epoch #8 is 0.4816666666666667\n",
            "Loss at epoch #8 is nan\n",
            "Time at epoch #8 is 589.7008900642395\n",
            "---------------FINISH epoch #8---------------\n",
            "---------------START epoch #9---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [3284.59908787526, 27956.784999788288, 18480.191966295486, 5599.051007874049, -6316.330885824692]\n",
            "bias [-2.3040371430190023e+20]\n",
            "Accuracy on test set at epoch #9 is 0.4816666666666667\n",
            "Loss at epoch #9 is nan\n",
            "Time at epoch #9 is 588.8102602958679\n",
            "---------------FINISH epoch #9---------------\n",
            "---------------START epoch #10---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [11800.222952456636, -1610.6514773114395, -9012.319295544901, -16038.831588735535, -14081.854491276532]\n",
            "bias [1.7425626546020255e+21]\n",
            "Accuracy on test set at epoch #10 is 0.5183333333333333\n",
            "Loss at epoch #10 is nan\n",
            "Time at epoch #10 is 588.6806800365448\n",
            "---------------FINISH epoch #10---------------\n",
            "---------------START epoch #11---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-10705.666183268046, 12915.125031860687, 15835.38168388331, 19571.865270440117, 35917.30938105967]\n",
            "bias [-2.6275771092826856e+20]\n",
            "Accuracy on test set at epoch #11 is 0.4816666666666667\n",
            "Loss at epoch #11 is nan\n",
            "Time at epoch #11 is 589.5855815410614\n",
            "---------------FINISH epoch #11---------------\n",
            "---------------START epoch #12---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-6372.812745902258, -12811.06890349821, -7499.663779038921, -12752.469702059396, -11535.872202513741]\n",
            "bias [3.5955611437937636e+20]\n",
            "Accuracy on test set at epoch #12 is 0.5183333333333333\n",
            "Loss at epoch #12 is nan\n",
            "Time at epoch #12 is 590.0969390869141\n",
            "---------------FINISH epoch #12---------------\n",
            "---------------START epoch #13---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [840.7649768830461, -7973.781387636598, 18368.75717305301, 14985.161884130584, -1011.5177776266682]\n",
            "bias [-5.57922929769492e+19]\n",
            "Accuracy on test set at epoch #13 is 0.4816666666666667\n",
            "Loss at epoch #13 is nan\n",
            "Time at epoch #13 is 588.6688296794891\n",
            "---------------FINISH epoch #13---------------\n",
            "---------------START epoch #14---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-1740.6640088831364, 15203.734059376928, 17309.767392981343, 303.4725579422907, 6851.421800488808]\n",
            "bias [-9.054167938913934e+20]\n",
            "Accuracy on test set at epoch #14 is 0.4816666666666667\n",
            "Loss at epoch #14 is nan\n",
            "Time at epoch #14 is 589.2646279335022\n",
            "---------------FINISH epoch #14---------------\n",
            "---------------START epoch #15---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-5266.585278785217, -22691.849404007917, 36494.49810604516, -23598.442825454906, -25772.537911582156]\n",
            "bias [-1.2891248817789235e+21]\n",
            "Accuracy on test set at epoch #15 is 0.4816666666666667\n",
            "Loss at epoch #15 is nan\n",
            "Time at epoch #15 is 589.4264805316925\n",
            "---------------FINISH epoch #15---------------\n",
            "---------------START epoch #16---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [3097.7954663859, -345.7618059682827, 3689.5006699200967, -23280.36560777304, 14887.207670583868]\n",
            "bias [-5.686418361900376e+20]\n",
            "Accuracy on test set at epoch #16 is 0.4816666666666667\n",
            "Loss at epoch #16 is nan\n",
            "Time at epoch #16 is 590.1986556053162\n",
            "---------------FINISH epoch #16---------------\n",
            "---------------START epoch #17---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-19954.923752496197, -977.6496908270365, -18101.1976319071, 1909.635021554794, 10594.794523396677]\n",
            "bias [7.264506021409676e+20]\n",
            "Accuracy on test set at epoch #17 is 0.5183333333333333\n",
            "Loss at epoch #17 is nan\n",
            "Time at epoch #17 is 589.0334646701813\n",
            "---------------FINISH epoch #17---------------\n",
            "---------------START epoch #18---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-8952.487050362019, 11121.056492278325, -3802.527212008646, -6069.777575992018, 17304.33409292871]\n",
            "bias [-2.1972393956162378e+20]\n",
            "Accuracy on test set at epoch #18 is 0.4816666666666667\n",
            "Loss at epoch #18 is nan\n",
            "Time at epoch #18 is 588.9966313838959\n",
            "---------------FINISH epoch #18---------------\n",
            "---------------START epoch #19---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [1639.5283434581113, 3933.2970247702633, 5308.152190873142, 6067.112691821217, -15081.886794647453]\n",
            "bias [2.953066462391268e+20]\n",
            "Accuracy on test set at epoch #19 is 0.5183333333333333\n",
            "Loss at epoch #19 is nan\n",
            "Time at epoch #19 is 587.9469397068024\n",
            "---------------FINISH epoch #19---------------\n",
            "---------------START epoch #20---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [3957.1137318750534, 19373.341064738812, -4963.107584836426, -2195.328629607191, -686.371471746465]\n",
            "bias [-9.295079366529724e+20]\n",
            "Accuracy on test set at epoch #20 is 0.4816666666666667\n",
            "Loss at epoch #20 is nan\n",
            "Time at epoch #20 is 588.1372320652008\n",
            "---------------FINISH epoch #20---------------\n",
            "---------------START epoch #21---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-13038.632799098996, -30268.338945426403, 11498.11587033327, -7792.6117237980025, -4775.454600694922]\n",
            "bias [5.7364584896455344e+20]\n",
            "Accuracy on test set at epoch #21 is 0.5183333333333333\n",
            "Loss at epoch #21 is nan\n",
            "Time at epoch #21 is 586.6028547286987\n",
            "---------------FINISH epoch #21---------------\n",
            "---------------START epoch #22---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [38965.9403361969, 16872.182631494336, -3891.7967835762042, 13111.648729363867, -17959.224925886094]\n",
            "bias [-9.659613821569294e+19]\n",
            "Accuracy on test set at epoch #22 is 0.4816666666666667\n",
            "Loss at epoch #22 is nan\n",
            "Time at epoch #22 is 587.4131996631622\n",
            "---------------FINISH epoch #22---------------\n",
            "---------------START epoch #23---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [1468.398513281599, -1631.5838034484095, 12152.007069531732, -3923.3044436840605, -16217.984740745827]\n",
            "bias [-6.621928718457812e+20]\n",
            "Accuracy on test set at epoch #23 is 0.4816666666666667\n",
            "Loss at epoch #23 is nan\n",
            "Time at epoch #23 is 588.4407141208649\n",
            "---------------FINISH epoch #23---------------\n",
            "---------------START epoch #24---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [5437.138452950727, -1158.085688063402, -2583.274433892001, -2991.9349586468543, -5770.890532167816]\n",
            "bias [-1.4453181857185763e+21]\n",
            "Accuracy on test set at epoch #24 is 0.4816666666666667\n",
            "Loss at epoch #24 is nan\n",
            "Time at epoch #24 is 588.2754068374634\n",
            "---------------FINISH epoch #24---------------\n",
            "---------------START epoch #25---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-4246.472350646891, 2156.4181542444985, 15835.561885356052, 12473.295573618992, 9318.947059464723]\n",
            "bias [-3.93809088805871e+20]\n",
            "Accuracy on test set at epoch #25 is 0.4816666666666667\n",
            "Loss at epoch #25 is nan\n",
            "Time at epoch #25 is 588.2019534111023\n",
            "---------------FINISH epoch #25---------------\n",
            "---------------START epoch #26---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [435.06219503418833, -1837.1398129916663, -31134.028463808583, -8731.324522636758, -4597.302013307606]\n",
            "bias [1.1243649254244028e+21]\n",
            "Accuracy on test set at epoch #26 is 0.5183333333333333\n",
            "Loss at epoch #26 is nan\n",
            "Time at epoch #26 is 588.3858506679535\n",
            "---------------FINISH epoch #26---------------\n",
            "---------------START epoch #27---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-8718.692581662726, 4850.588690281492, 4939.387232151811, 7066.27630837721, 28715.60483889377]\n",
            "bias [-1.3870090735795634e+21]\n",
            "Accuracy on test set at epoch #27 is 0.4816666666666667\n",
            "Loss at epoch #27 is nan\n",
            "Time at epoch #27 is 586.6176238059998\n",
            "---------------FINISH epoch #27---------------\n",
            "---------------START epoch #28---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-10682.713993225003, 27321.283477110483, -26881.696250872596, -2148.584799547595, -15310.382249669927]\n",
            "bias [-9.184975854959329e+18]\n",
            "Accuracy on test set at epoch #28 is 0.4816666666666667\n",
            "Loss at epoch #28 is nan\n",
            "Time at epoch #28 is 587.0526914596558\n",
            "---------------FINISH epoch #28---------------\n",
            "---------------START epoch #29---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-18945.171383994664, -16240.07164535796, 20756.527698883427, -758.6567976795905, 29075.358685423234]\n",
            "bias [3.5077576833954264e+20]\n",
            "Accuracy on test set at epoch #29 is 0.5183333333333333\n",
            "Loss at epoch #29 is nan\n",
            "Time at epoch #29 is 586.0890004634857\n",
            "---------------FINISH epoch #29---------------\n",
            "---------------START epoch #30---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-19111.32114142223, 2437.9895839659043, -35506.81591376398, -3188.8056553111946, -38764.268822740225]\n",
            "bias [1.393820965911863e+20]\n",
            "Accuracy on test set at epoch #30 is 0.5183333333333333\n",
            "Loss at epoch #30 is nan\n",
            "Time at epoch #30 is 586.2431037425995\n",
            "---------------FINISH epoch #30---------------\n",
            "---------------START epoch #31---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [9063.035714455422, 19482.344427621785, -1628.4573223252446, 18415.534540173183, -6341.1119517213165]\n",
            "bias [-1.358585307845346e+20]\n",
            "Accuracy on test set at epoch #31 is 0.4816666666666667\n",
            "Loss at epoch #31 is nan\n",
            "Time at epoch #31 is 586.0565755367279\n",
            "---------------FINISH epoch #31---------------\n",
            "---------------START epoch #32---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-10291.037920989718, 11086.060361335329, -7437.658867322591, 9150.209541718505, -8939.439058852247]\n",
            "bias [-9.166290326283614e+20]\n",
            "Accuracy on test set at epoch #32 is 0.4816666666666667\n",
            "Loss at epoch #32 is nan\n",
            "Time at epoch #32 is 591.2485029697418\n",
            "---------------FINISH epoch #32---------------\n",
            "---------------START epoch #33---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [31140.77382620822, 9888.479577001539, -1741.4561756134901, -6716.929680283477, -2505.3747411313307]\n",
            "bias [9.798052401386043e+20]\n",
            "Accuracy on test set at epoch #33 is 0.5183333333333333\n",
            "Loss at epoch #33 is nan\n",
            "Time at epoch #33 is 617.0483179092407\n",
            "---------------FINISH epoch #33---------------\n",
            "---------------START epoch #34---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-8679.308885667771, 2347.6582534702598, -8178.303587553225, 10219.886257908161, -1183.517874160143]\n",
            "bias [-1.5268134223118446e+21]\n",
            "Accuracy on test set at epoch #34 is 0.4816666666666667\n",
            "Loss at epoch #34 is nan\n",
            "Time at epoch #34 is 614.2115228176117\n",
            "---------------FINISH epoch #34---------------\n",
            "---------------START epoch #35---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-8417.587376194022, 10081.762163547672, 11579.253771889376, 30836.357743447188, 3983.0540759177507]\n",
            "bias [-3.1405975169294755e+20]\n",
            "Accuracy on test set at epoch #35 is 0.4816666666666667\n",
            "Loss at epoch #35 is nan\n",
            "Time at epoch #35 is 610.4077160358429\n",
            "---------------FINISH epoch #35---------------\n",
            "---------------START epoch #36---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-4810.706775128208, -10119.448763693872, 11116.692722851612, 16029.941458015383, -2291.650268546092]\n",
            "bias [4.1988110621908704e+20]\n",
            "Accuracy on test set at epoch #36 is 0.5183333333333333\n",
            "Loss at epoch #36 is nan\n",
            "Time at epoch #36 is 610.5858042240143\n",
            "---------------FINISH epoch #36---------------\n",
            "---------------START epoch #37---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [14630.13733523952, -23381.018451291475, 834.9667436988998, -7647.861727144987, 4330.74123132276]\n",
            "bias [4.78683584756893e+20]\n",
            "Accuracy on test set at epoch #37 is 0.5183333333333333\n",
            "Loss at epoch #37 is nan\n",
            "Time at epoch #37 is 607.6579716205597\n",
            "---------------FINISH epoch #37---------------\n",
            "---------------START epoch #38---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [-15795.432209917459, -12551.158707172366, -28134.454162948914, -11021.498403603702, 7343.615244273233]\n",
            "bias [9.816988730537438e+20]\n",
            "Accuracy on test set at epoch #38 is 0.5183333333333333\n",
            "Loss at epoch #38 is nan\n",
            "Time at epoch #38 is 608.9793155193329\n",
            "---------------FINISH epoch #38---------------\n",
            "---------------START epoch #39---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [11342.446556174764, -14616.149967799809, 21549.69791558182, 27541.911615314246, -2201.763510583067]\n",
            "bias [-7.796825059034438e+19]\n",
            "Accuracy on test set at epoch #39 is 0.4816666666666667\n",
            "Loss at epoch #39 is nan\n",
            "Time at epoch #39 is 605.9239354133606\n",
            "---------------FINISH epoch #39---------------\n",
            "---------------START epoch #40---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [18901.074096010445, -2705.436740544106, -3389.1487977334878, -1966.961634453428, -35292.601354313294]\n",
            "bias [6.706288769535536e+20]\n",
            "Accuracy on test set at epoch #40 is 0.5183333333333333\n",
            "Loss at epoch #40 is nan\n",
            "Time at epoch #40 is 608.5035471916199\n",
            "---------------FINISH epoch #40---------------\n",
            "---------------START epoch #41---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights [13671.849388103748, -8605.610287121077, 19137.77124485465, 771.6571211670162, -18573.580026301992]\n",
            "bias [7.686385074177927e+20]\n",
            "Accuracy on test set at epoch #41 is 0.5183333333333333\n",
            "Loss at epoch #41 is nan\n",
            "Time at epoch #41 is 612.0843014717102\n",
            "---------------FINISH epoch #41---------------\n",
            "---------------START epoch #42---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_QyhChvqi4h",
        "outputId": "1a38a88b-d7a7-46df-bf14-28d1247aa936"
      },
      "source": [
        "lrgd = LRGD(context=ctx_training, n_features=len(HRF_x_train[0,:]), epoch=100, lr=0.01)\n",
        "lrgd.train(HRF_x_train, HRF_y_train, HRF_x_test, HRF_y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------START epoch #1---------------\n",
            "weights [ 0.00469658 -0.00177478  0.01403608  0.02071709 -0.00177478]\n",
            "bias [-7.14285714e-05]\n",
            "Accuracy on test set at epoch #1 is 0.52\n",
            "Loss at epoch #1 is 0.634350962924504\n",
            "Time at epoch #1 is 0.020824670791625977\n",
            "---------------FINISH epoch #1---------------\n",
            "---------------START epoch #2---------------\n",
            "weights [ 0.00845477 -0.00273569  0.02549967  0.0384497  -0.00273569]\n",
            "bias [-0.00051082]\n",
            "Accuracy on test set at epoch #2 is 0.52\n",
            "Loss at epoch #2 is 0.5948445708150701\n",
            "Time at epoch #2 is 0.022139549255371094\n",
            "---------------FINISH epoch #2---------------\n",
            "---------------START epoch #3---------------\n",
            "weights [ 0.01144343 -0.00303203  0.03485264  0.05372976 -0.00303203]\n",
            "bias [-0.00125107]\n",
            "Accuracy on test set at epoch #3 is 0.5233333333333333\n",
            "Loss at epoch #3 is 0.5678545782212209\n",
            "Time at epoch #3 is 0.022730112075805664\n",
            "---------------FINISH epoch #3---------------\n",
            "---------------START epoch #4---------------\n",
            "weights [ 0.01380382 -0.0027885   0.04248155  0.06700282 -0.0027885 ]\n",
            "bias [-0.00223612]\n",
            "Accuracy on test set at epoch #4 is 0.5266666666666666\n",
            "Loss at epoch #4 is 0.5489284905426062\n",
            "Time at epoch #4 is 0.010858774185180664\n",
            "---------------FINISH epoch #4---------------\n",
            "---------------START epoch #5---------------\n",
            "weights [ 0.0156513  -0.00210682  0.04870205  0.07863276 -0.00210682]\n",
            "bias [-0.00342025]\n",
            "Accuracy on test set at epoch #5 is 0.5266666666666666\n",
            "Loss at epoch #5 is 0.5352177781039644\n",
            "Time at epoch #5 is 0.02285933494567871\n",
            "---------------FINISH epoch #5---------------\n",
            "---------------START epoch #6---------------\n",
            "weights [ 0.017079   -0.00106915  0.05376898  0.08891331 -0.00106915]\n",
            "bias [-0.00476647]\n",
            "Accuracy on test set at epoch #6 is 0.53\n",
            "Loss at epoch #6 is 0.524912555461229\n",
            "Time at epoch #6 is 0.020508766174316406\n",
            "---------------FINISH epoch #6---------------\n",
            "---------------START epoch #7---------------\n",
            "weights [0.01816186 0.00025839 0.05788737 0.09808068 0.00025839]\n",
            "bias [-0.00624496]\n",
            "Accuracy on test set at epoch #7 is 0.5333333333333333\n",
            "Loss at epoch #7 is 0.5168582948728161\n",
            "Time at epoch #7 is 0.0216982364654541\n",
            "---------------FINISH epoch #7---------------\n",
            "---------------START epoch #8---------------\n",
            "weights [0.01896021 0.00182252 0.06122226 0.10632492 0.00182252]\n",
            "bias [-0.00783167]\n",
            "Accuracy on test set at epoch #8 is 0.5333333333333333\n",
            "Loss at epoch #8 is 0.5103116341830821\n",
            "Time at epoch #8 is 0.013071537017822266\n",
            "---------------FINISH epoch #8---------------\n",
            "---------------START epoch #9---------------\n",
            "weights [0.01952278 0.00358015 0.06390696 0.11379946 0.00358015]\n",
            "bias [-0.0095071]\n",
            "Accuracy on test set at epoch #9 is 0.54\n",
            "Loss at epoch #9 is 0.504788408618934\n",
            "Time at epoch #9 is 0.02171468734741211\n",
            "---------------FINISH epoch #9---------------\n",
            "---------------START epoch #10---------------\n",
            "weights [0.01988913 0.00549628 0.06604966 0.12062872 0.00549628]\n",
            "bias [-0.0112554]\n",
            "Accuracy on test set at epoch #10 is 0.5433333333333333\n",
            "Loss at epoch #10 is 0.4999694851480092\n",
            "Time at epoch #10 is 0.02691483497619629\n",
            "---------------FINISH epoch #10---------------\n",
            "---------------START epoch #11---------------\n",
            "weights [0.02009154 0.00754233 0.0677386  0.12691416 0.00754233]\n",
            "bias [-0.0130636]\n",
            "Accuracy on test set at epoch #11 is 0.5433333333333333\n",
            "Loss at epoch #11 is 0.4956420953372107\n",
            "Time at epoch #11 is 0.02116703987121582\n",
            "---------------FINISH epoch #11---------------\n",
            "---------------START epoch #12---------------\n",
            "weights [0.02015649 0.00969484 0.06904617 0.13273894 0.00969484]\n",
            "bias [-0.01492105]\n",
            "Accuracy on test set at epoch #12 is 0.5433333333333333\n",
            "Loss at epoch #12 is 0.4916629048681609\n",
            "Time at epoch #12 is 0.02295088768005371\n",
            "---------------FINISH epoch #12---------------\n",
            "---------------START epoch #13---------------\n",
            "weights [0.02010581 0.01193443 0.07003207 0.13817163 0.01193443]\n",
            "bias [-0.01681893]\n",
            "Accuracy on test set at epoch #13 is 0.55\n",
            "Loss at epoch #13 is 0.4879344724295822\n",
            "Time at epoch #13 is 0.017125606536865234\n",
            "---------------FINISH epoch #13---------------\n",
            "---------------START epoch #14---------------\n",
            "weights [0.01995762 0.01424504 0.07074582 0.14326909 0.01424504]\n",
            "bias [-0.01874992]\n",
            "Accuracy on test set at epoch #14 is 0.55\n",
            "Loss at epoch #14 is 0.4843900431350705\n",
            "Time at epoch #14 is 0.019991636276245117\n",
            "---------------FINISH epoch #14---------------\n",
            "---------------START epoch #15---------------\n",
            "weights [0.01972702 0.01661326 0.07122869 0.14807868 0.01661326]\n",
            "bias [-0.0207079]\n",
            "Accuracy on test set at epoch #15 is 0.5533333333333333\n",
            "Loss at epoch #15 is 0.48098359589381434\n",
            "Time at epoch #15 is 0.0222012996673584\n",
            "---------------FINISH epoch #15---------------\n",
            "---------------START epoch #16---------------\n",
            "weights [0.01942664 0.01902788 0.07151521 0.15264004 0.01902788]\n",
            "bias [-0.02268777]\n",
            "Accuracy on test set at epoch #16 is 0.56\n",
            "Loss at epoch #16 is 0.4776832475469376\n",
            "Time at epoch #16 is 0.021142244338989258\n",
            "---------------FINISH epoch #16---------------\n",
            "---------------START epoch #17---------------\n",
            "weights [0.01906713 0.02147946 0.07163444 0.15698652 0.02147946]\n",
            "bias [-0.0246852]\n",
            "Accuracy on test set at epoch #17 is 0.56\n",
            "Loss at epoch #17 is 0.4744668306570257\n",
            "Time at epoch #17 is 0.022339582443237305\n",
            "---------------FINISH epoch #17---------------\n",
            "---------------START epoch #18---------------\n",
            "weights [0.01865745 0.02396006 0.0716109  0.16114627 0.02396006]\n",
            "bias [-0.02669657]\n",
            "Accuracy on test set at epoch #18 is 0.5733333333333334\n",
            "Loss at epoch #18 is 0.47131889723427356\n",
            "Time at epoch #18 is 0.021512985229492188\n",
            "---------------FINISH epoch #18---------------\n",
            "---------------START epoch #19---------------\n",
            "weights [0.0182052  0.02646293 0.07146537 0.16514315 0.02646293]\n",
            "bias [-0.02871878]\n",
            "Accuracy on test set at epoch #19 is 0.5833333333333334\n",
            "Loss at epoch #19 is 0.4682286694444632\n",
            "Time at epoch #19 is 0.01993393898010254\n",
            "---------------FINISH epoch #19---------------\n",
            "---------------START epoch #20---------------\n",
            "weights [0.01771685 0.02898238 0.07121551 0.16899744 0.02898238]\n",
            "bias [-0.03074923]\n",
            "Accuracy on test set at epoch #20 is 0.5966666666666667\n",
            "Loss at epoch #20 is 0.46518862643691256\n",
            "Time at epoch #20 is 0.02365589141845703\n",
            "---------------FINISH epoch #20---------------\n",
            "---------------START epoch #21---------------\n",
            "weights [0.0171979  0.03151354 0.07087637 0.17272646 0.03151354]\n",
            "bias [-0.03278569]\n",
            "Accuracy on test set at epoch #21 is 0.6\n",
            "Loss at epoch #21 is 0.46219352297016597\n",
            "Time at epoch #21 is 0.015812158584594727\n",
            "---------------FINISH epoch #21---------------\n",
            "---------------START epoch #22---------------\n",
            "weights [0.01665305 0.03405228 0.07046081 0.17634501 0.03405228]\n",
            "bias [-0.03482626]\n",
            "Accuracy on test set at epoch #22 is 0.61\n",
            "Loss at epoch #22 is 0.45923970393082364\n",
            "Time at epoch #22 is 0.010527849197387695\n",
            "---------------FINISH epoch #22---------------\n",
            "---------------START epoch #23---------------\n",
            "weights [0.01608633 0.03659507 0.06997985 0.17986578 0.03659507]\n",
            "bias [-0.03686931]\n",
            "Accuracy on test set at epoch #23 is 0.62\n",
            "Loss at epoch #23 is 0.45632462333827545\n",
            "Time at epoch #23 is 0.010845661163330078\n",
            "---------------FINISH epoch #23---------------\n",
            "---------------START epoch #24---------------\n",
            "weights [0.01550119 0.03913889 0.06944295 0.18329969 0.03913889]\n",
            "bias [-0.03891346]\n",
            "Accuracy on test set at epoch #24 is 0.62\n",
            "Loss at epoch #24 is 0.4534465057210924\n",
            "Time at epoch #24 is 0.011027812957763672\n",
            "---------------FINISH epoch #24---------------\n",
            "---------------START epoch #25---------------\n",
            "weights [0.01490062 0.04168116 0.06885824 0.18665613 0.04168116]\n",
            "bias [-0.04095753]\n",
            "Accuracy on test set at epoch #25 is 0.6266666666666667\n",
            "Loss at epoch #25 is 0.4506041072570183\n",
            "Time at epoch #25 is 0.010744094848632812\n",
            "---------------FINISH epoch #25---------------\n",
            "---------------START epoch #26---------------\n",
            "weights [0.01428715 0.04421967 0.06823271 0.18994323 0.04421967]\n",
            "bias [-0.04300049]\n",
            "Accuracy on test set at epoch #26 is 0.63\n",
            "Loss at epoch #26 is 0.4477965471982016\n",
            "Time at epoch #26 is 0.01949286460876465\n",
            "---------------FINISH epoch #26---------------\n",
            "---------------START epoch #27---------------\n",
            "weights [0.013663   0.04675255 0.06757242 0.193168   0.04675255]\n",
            "bias [-0.04504147]\n",
            "Accuracy on test set at epoch #27 is 0.6366666666666667\n",
            "Loss at epoch #27 is 0.4450231890300522\n",
            "Time at epoch #27 is 0.025144338607788086\n",
            "---------------FINISH epoch #27---------------\n",
            "---------------START epoch #28---------------\n",
            "weights [0.01303006 0.04927816 0.06688258 0.19633651 0.04927816]\n",
            "bias [-0.04707973]\n",
            "Accuracy on test set at epoch #28 is 0.6466666666666666\n",
            "Loss at epoch #28 is 0.44228355693790167\n",
            "Time at epoch #28 is 0.020935535430908203\n",
            "---------------FINISH epoch #28---------------\n",
            "---------------START epoch #29---------------\n",
            "weights [0.01238999 0.05179514 0.06616769 0.19945405 0.05179514]\n",
            "bias [-0.04911462]\n",
            "Accuracy on test set at epoch #29 is 0.6566666666666666\n",
            "Loss at epoch #29 is 0.4395772773953339\n",
            "Time at epoch #29 is 0.023184776306152344\n",
            "---------------FINISH epoch #29---------------\n",
            "---------------START epoch #30---------------\n",
            "weights [0.01174419 0.0543023  0.06543166 0.20252518 0.0543023 ]\n",
            "bias [-0.0511456]\n",
            "Accuracy on test set at epoch #30 is 0.6733333333333333\n",
            "Loss at epoch #30 is 0.4369040386452587\n",
            "Time at epoch #30 is 0.02163863182067871\n",
            "---------------FINISH epoch #30---------------\n",
            "---------------START epoch #31---------------\n",
            "weights [0.01109389 0.05679864 0.06467786 0.2055539  0.05679864]\n",
            "bias [-0.0531722]\n",
            "Accuracy on test set at epoch #31 is 0.68\n",
            "Loss at epoch #31 is 0.43426356292197876\n",
            "Time at epoch #31 is 0.02213907241821289\n",
            "---------------FINISH epoch #31---------------\n",
            "---------------START epoch #32---------------\n",
            "weights [0.01044015 0.05928331 0.06390923 0.20854368 0.05928331]\n",
            "bias [-0.05519401]\n",
            "Accuracy on test set at epoch #32 is 0.6866666666666666\n",
            "Loss at epoch #32 is 0.43165558773070617\n",
            "Time at epoch #32 is 0.010274887084960938\n",
            "---------------FINISH epoch #32---------------\n",
            "---------------START epoch #33---------------\n",
            "weights [0.0097839  0.06175557 0.0631283  0.21149754 0.06175557]\n",
            "bias [-0.05721071]\n",
            "Accuracy on test set at epoch #33 is 0.69\n",
            "Loss at epoch #33 is 0.42907985354449224\n",
            "Time at epoch #33 is 0.011471033096313477\n",
            "---------------FINISH epoch #33---------------\n",
            "---------------START epoch #34---------------\n",
            "weights [0.00912593 0.06421481 0.06233728 0.21441814 0.06421481]\n",
            "bias [-0.05922199]\n",
            "Accuracy on test set at epoch #34 is 0.6966666666666667\n",
            "Loss at epoch #34 is 0.42653609602372966\n",
            "Time at epoch #34 is 0.011337757110595703\n",
            "---------------FINISH epoch #34---------------\n",
            "---------------START epoch #35---------------\n",
            "weights [0.00846692 0.06666052 0.06153807 0.21730779 0.06666052]\n",
            "bias [-0.0612276]\n",
            "Accuracy on test set at epoch #35 is 0.71\n",
            "Loss at epoch #35 is 0.42402404139769956\n",
            "Time at epoch #35 is 0.012456417083740234\n",
            "---------------FINISH epoch #35---------------\n",
            "---------------START epoch #36---------------\n",
            "weights [0.00780748 0.06909227 0.06073234 0.22016851 0.06909227]\n",
            "bias [-0.06322736]\n",
            "Accuracy on test set at epoch #36 is 0.7166666666666667\n",
            "Loss at epoch #36 is 0.4215434040319844\n",
            "Time at epoch #36 is 0.02163100242614746\n",
            "---------------FINISH epoch #36---------------\n",
            "---------------START epoch #37---------------\n",
            "weights [0.00714812 0.07150969 0.05992152 0.2230021  0.07150969]\n",
            "bias [-0.06522107]\n",
            "Accuracy on test set at epoch #37 is 0.7266666666666667\n",
            "Loss at epoch #37 is 0.41909388548273974\n",
            "Time at epoch #37 is 0.020143747329711914\n",
            "---------------FINISH epoch #37---------------\n",
            "---------------START epoch #38---------------\n",
            "weights [0.00648928 0.07391248 0.05910687 0.2258101  0.07391248]\n",
            "bias [-0.06720859]\n",
            "Accuracy on test set at epoch #38 is 0.7333333333333333\n",
            "Loss at epoch #38 is 0.4166751745390326\n",
            "Time at epoch #38 is 0.021065950393676758\n",
            "---------------FINISH epoch #38---------------\n",
            "---------------START epoch #39---------------\n",
            "weights [0.00583134 0.07630041 0.05828946 0.2285939  0.07630041]\n",
            "bias [-0.06918981]\n",
            "Accuracy on test set at epoch #39 is 0.74\n",
            "Loss at epoch #39 is 0.41428694789919746\n",
            "Time at epoch #39 is 0.02238631248474121\n",
            "---------------FINISH epoch #39---------------\n",
            "---------------START epoch #40---------------\n",
            "weights [0.00517465 0.07867329 0.05747024 0.23135471 0.07867329]\n",
            "bias [-0.07116462]\n",
            "Accuracy on test set at epoch #40 is 0.7433333333333333\n",
            "Loss at epoch #40 is 0.41192887123180444\n",
            "Time at epoch #40 is 0.02234673500061035\n",
            "---------------FINISH epoch #40---------------\n",
            "---------------START epoch #41---------------\n",
            "weights [0.00451949 0.08103095 0.05665003 0.23409359 0.08103095]\n",
            "bias [-0.07313295]\n",
            "Accuracy on test set at epoch #41 is 0.75\n",
            "Loss at epoch #41 is 0.4096006004473978\n",
            "Time at epoch #41 is 0.021546363830566406\n",
            "---------------FINISH epoch #41---------------\n",
            "---------------START epoch #42---------------\n",
            "weights [0.0038661  0.08337328 0.05582953 0.23681151 0.08337328]\n",
            "bias [-0.07509472]\n",
            "Accuracy on test set at epoch #42 is 0.7566666666666667\n",
            "Loss at epoch #42 is 0.40730178306160986\n",
            "Time at epoch #42 is 0.0221402645111084\n",
            "---------------FINISH epoch #42---------------\n",
            "---------------START epoch #43---------------\n",
            "weights [0.0032147  0.0857002  0.05500936 0.23950928 0.0857002 ]\n",
            "bias [-0.0770499]\n",
            "Accuracy on test set at epoch #43 is 0.7633333333333333\n",
            "Loss at epoch #43 is 0.40503205956935623\n",
            "Time at epoch #43 is 0.01996588706970215\n",
            "---------------FINISH epoch #43---------------\n",
            "---------------START epoch #44---------------\n",
            "weights [0.00256547 0.08801164 0.05419005 0.24218767 0.08801164]\n",
            "bias [-0.07899844]\n",
            "Accuracy on test set at epoch #44 is 0.7633333333333333\n",
            "Loss at epoch #44 is 0.4027910647777409\n",
            "Time at epoch #44 is 0.020578622817993164\n",
            "---------------FINISH epoch #44---------------\n",
            "---------------START epoch #45---------------\n",
            "weights [0.00191857 0.09030758 0.05337206 0.24484733 0.09030758]\n",
            "bias [-0.08094031]\n",
            "Accuracy on test set at epoch #45 is 0.7666666666666667\n",
            "Loss at epoch #45 is 0.40057842906510804\n",
            "Time at epoch #45 is 0.02256155014038086\n",
            "---------------FINISH epoch #45---------------\n",
            "---------------START epoch #46---------------\n",
            "weights [0.00127412 0.09258799 0.05255579 0.24748886 0.09258799]\n",
            "bias [-0.08287551]\n",
            "Accuracy on test set at epoch #46 is 0.77\n",
            "Loss at epoch #46 is 0.3983937795475874\n",
            "Time at epoch #46 is 0.011848926544189453\n",
            "---------------FINISH epoch #46---------------\n",
            "---------------START epoch #47---------------\n",
            "weights [0.00063225 0.09485288 0.05174158 0.25011278 0.09485288]\n",
            "bias [-0.08480402]\n",
            "Accuracy on test set at epoch #47 is 0.78\n",
            "Loss at epoch #47 is 0.39623674114411583\n",
            "Time at epoch #47 is 0.011652708053588867\n",
            "---------------FINISH epoch #47---------------\n",
            "---------------START epoch #48---------------\n",
            "weights [-6.95622584e-06  9.71022755e-02  5.09297216e-02  2.52719575e-01\n",
            "  9.71022755e-02]\n",
            "bias [-0.08672584]\n",
            "Accuracy on test set at epoch #48 is 0.7866666666666666\n",
            "Loss at epoch #48 is 0.39410693753747345\n",
            "Time at epoch #48 is 0.02104330062866211\n",
            "---------------FINISH epoch #48---------------\n",
            "---------------START epoch #49---------------\n",
            "weights [-0.00064341  0.0993362   0.05012048  0.25530967  0.0993362 ]\n",
            "bias [-0.08864098]\n",
            "Accuracy on test set at epoch #49 is 0.79\n",
            "Loss at epoch #49 is 0.39200399203320363\n",
            "Time at epoch #49 is 0.01906752586364746\n",
            "---------------FINISH epoch #49---------------\n",
            "---------------START epoch #50---------------\n",
            "weights [-0.00127705  0.10155469  0.04931406  0.25788345  0.10155469]\n",
            "bias [-0.09054943]\n",
            "Accuracy on test set at epoch #50 is 0.7933333333333333\n",
            "Loss at epoch #50 is 0.38992752832105004\n",
            "Time at epoch #50 is 0.023357629776000977\n",
            "---------------FINISH epoch #50---------------\n",
            "---------------START epoch #51---------------\n",
            "weights [-0.00190782  0.10375781  0.04851067  0.26044127  0.10375781]\n",
            "bias [-0.09245123]\n",
            "Accuracy on test set at epoch #51 is 0.8\n",
            "Loss at epoch #51 is 0.3878771711451764\n",
            "Time at epoch #51 is 0.02159404754638672\n",
            "---------------FINISH epoch #51---------------\n",
            "---------------START epoch #52---------------\n",
            "weights [-0.00253567  0.10594562  0.04771045  0.26298344  0.10594562]\n",
            "bias [-0.09434638]\n",
            "Accuracy on test set at epoch #52 is 0.8033333333333333\n",
            "Loss at epoch #52 is 0.38585254689029186\n",
            "Time at epoch #52 is 0.021172046661376953\n",
            "---------------FINISH epoch #52---------------\n",
            "---------------START epoch #53---------------\n",
            "weights [-0.00316057  0.10811818  0.04691354  0.26551025  0.10811818]\n",
            "bias [-0.09623491]\n",
            "Accuracy on test set at epoch #53 is 0.8066666666666666\n",
            "Loss at epoch #53 is 0.3838532840911323\n",
            "Time at epoch #53 is 0.022260665893554688\n",
            "---------------FINISH epoch #53---------------\n",
            "---------------START epoch #54---------------\n",
            "weights [-0.00378248  0.11027558  0.04612005  0.26802197  0.11027558]\n",
            "bias [-0.09811685]\n",
            "Accuracy on test set at epoch #54 is 0.8066666666666666\n",
            "Loss at epoch #54 is 0.3818790138727009\n",
            "Time at epoch #54 is 0.021866559982299805\n",
            "---------------FINISH epoch #54---------------\n",
            "---------------START epoch #55---------------\n",
            "weights [-0.00440138  0.11241789  0.04533009  0.27051885  0.11241789]\n",
            "bias [-0.09999221]\n",
            "Accuracy on test set at epoch #55 is 0.8066666666666666\n",
            "Loss at epoch #55 is 0.3799293703284104\n",
            "Time at epoch #55 is 0.020269393920898438\n",
            "---------------FINISH epoch #55---------------\n",
            "---------------START epoch #56---------------\n",
            "weights [-0.00501726  0.1145452   0.04454373  0.27300111  0.1145452 ]\n",
            "bias [-0.10186103]\n",
            "Accuracy on test set at epoch #56 is 0.8066666666666666\n",
            "Loss at epoch #56 is 0.3780039908428467\n",
            "Time at epoch #56 is 0.010443449020385742\n",
            "---------------FINISH epoch #56---------------\n",
            "---------------START epoch #57---------------\n",
            "weights [-0.0056301   0.11665759  0.04376105  0.27546896  0.11665759]\n",
            "bias [-0.10372334]\n",
            "Accuracy on test set at epoch #57 is 0.8066666666666666\n",
            "Loss at epoch #57 is 0.3761025163653893\n",
            "Time at epoch #57 is 0.008851766586303711\n",
            "---------------FINISH epoch #57---------------\n",
            "---------------START epoch #58---------------\n",
            "weights [-0.00623989  0.11875517  0.04298208  0.27792259  0.11875517]\n",
            "bias [-0.10557917]\n",
            "Accuracy on test set at epoch #58 is 0.8133333333333334\n",
            "Loss at epoch #58 is 0.3742245916403916\n",
            "Time at epoch #58 is 0.010034799575805664\n",
            "---------------FINISH epoch #58---------------\n",
            "---------------START epoch #59---------------\n",
            "weights [-0.00684664  0.12083803  0.04220688  0.28036219  0.12083803]\n",
            "bias [-0.10742856]\n",
            "Accuracy on test set at epoch #59 is 0.82\n",
            "Loss at epoch #59 is 0.37236986539909117\n",
            "Time at epoch #59 is 0.0289306640625\n",
            "---------------FINISH epoch #59---------------\n",
            "---------------START epoch #60---------------\n",
            "weights [-0.00745034  0.12290627  0.04143549  0.28278793  0.12290627]\n",
            "bias [-0.10927154]\n",
            "Accuracy on test set at epoch #60 is 0.82\n",
            "Loss at epoch #60 is 0.3705379905179034\n",
            "Time at epoch #60 is 0.022785186767578125\n",
            "---------------FINISH epoch #60---------------\n",
            "---------------START epoch #61---------------\n",
            "weights [-0.00805099  0.12495998  0.04066792  0.28519996  0.12495998]\n",
            "bias [-0.11110815]\n",
            "Accuracy on test set at epoch #61 is 0.82\n",
            "Loss at epoch #61 is 0.3687286241472597\n",
            "Time at epoch #61 is 0.019150972366333008\n",
            "---------------FINISH epoch #61---------------\n",
            "---------------START epoch #62---------------\n",
            "weights [-0.0086486   0.12699927  0.0399042   0.28759844  0.12699927]\n",
            "bias [-0.11293843]\n",
            "Accuracy on test set at epoch #62 is 0.82\n",
            "Loss at epoch #62 is 0.36694142781468736\n",
            "Time at epoch #62 is 0.011485099792480469\n",
            "---------------FINISH epoch #62---------------\n",
            "---------------START epoch #63---------------\n",
            "weights [-0.00924317  0.12902424  0.03914434  0.28998352  0.12902424]\n",
            "bias [-0.1147624]\n",
            "Accuracy on test set at epoch #63 is 0.82\n",
            "Loss at epoch #63 is 0.3651760675054148\n",
            "Time at epoch #63 is 0.01953125\n",
            "---------------FINISH epoch #63---------------\n",
            "---------------START epoch #64---------------\n",
            "weights [-0.00983472  0.131035    0.03838835  0.29235532  0.131035  ]\n",
            "bias [-0.11658012]\n",
            "Accuracy on test set at epoch #64 is 0.82\n",
            "Loss at epoch #64 is 0.3634322137233956\n",
            "Time at epoch #64 is 0.012788772583007812\n",
            "---------------FINISH epoch #64---------------\n",
            "---------------START epoch #65---------------\n",
            "weights [-0.01042325  0.13303165  0.03763624  0.29471399  0.13303165]\n",
            "bias [-0.11839162]\n",
            "Accuracy on test set at epoch #65 is 0.8266666666666667\n",
            "Loss at epoch #65 is 0.36170954153530244\n",
            "Time at epoch #65 is 0.022079944610595703\n",
            "---------------FINISH epoch #65---------------\n",
            "---------------START epoch #66---------------\n",
            "weights [-0.01100878  0.13501429  0.036888    0.29705965  0.13501429]\n",
            "bias [-0.12019693]\n",
            "Accuracy on test set at epoch #66 is 0.83\n",
            "Loss at epoch #66 is 0.36000773059973273\n",
            "Time at epoch #66 is 0.024460792541503906\n",
            "---------------FINISH epoch #66---------------\n",
            "---------------START epoch #67---------------\n",
            "weights [-0.01159131  0.13698304  0.03614362  0.29939241  0.13698304]\n",
            "bias [-0.1219961]\n",
            "Accuracy on test set at epoch #67 is 0.8333333333333334\n",
            "Loss at epoch #67 is 0.35832646518359007\n",
            "Time at epoch #67 is 0.020254850387573242\n",
            "---------------FINISH epoch #67---------------\n",
            "---------------START epoch #68---------------\n",
            "weights [-0.01217088  0.138938    0.03540311  0.30171241  0.138938  ]\n",
            "bias [-0.12378917]\n",
            "Accuracy on test set at epoch #68 is 0.8433333333333334\n",
            "Loss at epoch #68 is 0.35666543416736307\n",
            "Time at epoch #68 is 0.02092599868774414\n",
            "---------------FINISH epoch #68---------------\n",
            "---------------START epoch #69---------------\n",
            "weights [-0.01274747  0.14087927  0.03466645  0.30401974  0.14087927]\n",
            "bias [-0.12557618]\n",
            "Accuracy on test set at epoch #69 is 0.8466666666666667\n",
            "Loss at epoch #69 is 0.35502433104080694\n",
            "Time at epoch #69 is 0.01822209358215332\n",
            "---------------FINISH epoch #69---------------\n",
            "---------------START epoch #70---------------\n",
            "weights [-0.01332113  0.14280697  0.03393363  0.30631452  0.14280697]\n",
            "bias [-0.12735716]\n",
            "Accuracy on test set at epoch #70 is 0.8466666666666667\n",
            "Loss at epoch #70 is 0.35340285389034115\n",
            "Time at epoch #70 is 0.02073502540588379\n",
            "---------------FINISH epoch #70---------------\n",
            "---------------START epoch #71---------------\n",
            "weights [-0.01389185  0.1447212   0.03320464  0.30859685  0.1447212 ]\n",
            "bias [-0.12913216]\n",
            "Accuracy on test set at epoch #71 is 0.8466666666666667\n",
            "Loss at epoch #71 is 0.35180070537931296\n",
            "Time at epoch #71 is 0.020818710327148438\n",
            "---------------FINISH epoch #71---------------\n",
            "---------------START epoch #72---------------\n",
            "weights [-0.01445966  0.14662206  0.03247946  0.31086685  0.14662206]\n",
            "bias [-0.13090121]\n",
            "Accuracy on test set at epoch #72 is 0.8466666666666667\n",
            "Loss at epoch #72 is 0.35021759272212616\n",
            "Time at epoch #72 is 0.021310806274414062\n",
            "---------------FINISH epoch #72---------------\n",
            "---------------START epoch #73---------------\n",
            "weights [-0.01502458  0.14850966  0.03175807  0.31312461  0.14850966]\n",
            "bias [-0.13266436]\n",
            "Accuracy on test set at epoch #73 is 0.8466666666666667\n",
            "Loss at epoch #73 is 0.3486532276531097\n",
            "Time at epoch #73 is 0.020279407501220703\n",
            "---------------FINISH epoch #73---------------\n",
            "---------------START epoch #74---------------\n",
            "weights [-0.01558662  0.15038411  0.03104046  0.31537023  0.15038411]\n",
            "bias [-0.13442165]\n",
            "Accuracy on test set at epoch #74 is 0.8466666666666667\n",
            "Loss at epoch #74 is 0.3471073263908866\n",
            "Time at epoch #74 is 0.022481441497802734\n",
            "---------------FINISH epoch #74---------------\n",
            "---------------START epoch #75---------------\n",
            "weights [-0.0161458   0.15224551  0.0303266   0.3176038   0.15224551]\n",
            "bias [-0.13617311]\n",
            "Accuracy on test set at epoch #75 is 0.8466666666666667\n",
            "Loss at epoch #75 is 0.3455796095989072\n",
            "Time at epoch #75 is 0.020656108856201172\n",
            "---------------FINISH epoch #75---------------\n",
            "---------------START epoch #76---------------\n",
            "weights [-0.01670214  0.15409397  0.02961648  0.31982543  0.15409397]\n",
            "bias [-0.13791878]\n",
            "Accuracy on test set at epoch #76 is 0.85\n",
            "Loss at epoch #76 is 0.3440698023427247\n",
            "Time at epoch #76 is 0.014396905899047852\n",
            "---------------FINISH epoch #76---------------\n",
            "---------------START epoch #77---------------\n",
            "weights [-0.01725565  0.15592959  0.02891007  0.32203521  0.15592959]\n",
            "bias [-0.1396587]\n",
            "Accuracy on test set at epoch #77 is 0.8533333333333334\n",
            "Loss at epoch #77 is 0.3425776340445172\n",
            "Time at epoch #77 is 0.015747547149658203\n",
            "---------------FINISH epoch #77---------------\n",
            "---------------START epoch #78---------------\n",
            "weights [-0.01780637  0.15775247  0.02820736  0.32423323  0.15775247]\n",
            "bias [-0.14139292]\n",
            "Accuracy on test set at epoch #78 is 0.8533333333333334\n",
            "Loss at epoch #78 is 0.341102838435296\n",
            "Time at epoch #78 is 0.010554075241088867\n",
            "---------------FINISH epoch #78---------------\n",
            "---------------START epoch #79---------------\n",
            "weights [-0.0183543   0.15956273  0.02750832  0.32641957  0.15956273]\n",
            "bias [-0.14312147]\n",
            "Accuracy on test set at epoch #79 is 0.8533333333333334\n",
            "Loss at epoch #79 is 0.33964515350518376\n",
            "Time at epoch #79 is 0.008687019348144531\n",
            "---------------FINISH epoch #79---------------\n",
            "---------------START epoch #80---------------\n",
            "weights [-0.01889946  0.16136045  0.02681292  0.32859433  0.16136045]\n",
            "bias [-0.14484439]\n",
            "Accuracy on test set at epoch #80 is 0.8533333333333334\n",
            "Loss at epoch #80 is 0.3382043214520955\n",
            "Time at epoch #80 is 0.008623361587524414\n",
            "---------------FINISH epoch #80---------------\n",
            "---------------START epoch #81---------------\n",
            "weights [-0.01944188  0.16314575  0.02612115  0.3307576   0.16314575]\n",
            "bias [-0.14656171]\n",
            "Accuracy on test set at epoch #81 is 0.8533333333333334\n",
            "Loss at epoch #81 is 0.33678008862911546\n",
            "Time at epoch #81 is 0.008533000946044922\n",
            "---------------FINISH epoch #81---------------\n",
            "---------------START epoch #82---------------\n",
            "weights [-0.01998157  0.16491872  0.02543299  0.33290947  0.16491872]\n",
            "bias [-0.14827348]\n",
            "Accuracy on test set at epoch #82 is 0.86\n",
            "Loss at epoch #82 is 0.33537220549082464\n",
            "Time at epoch #82 is 0.00881338119506836\n",
            "---------------FINISH epoch #82---------------\n",
            "---------------START epoch #83---------------\n",
            "weights [-0.02051855  0.16667946  0.0247484   0.33505001  0.16667946]\n",
            "bias [-0.14997974]\n",
            "Accuracy on test set at epoch #83 is 0.86\n",
            "Loss at epoch #83 is 0.33398042653879967\n",
            "Time at epoch #83 is 0.008848905563354492\n",
            "---------------FINISH epoch #83---------------\n",
            "---------------START epoch #84---------------\n",
            "weights [-0.02105284  0.16842808  0.02406737  0.33717932  0.16842808]\n",
            "bias [-0.15168052]\n",
            "Accuracy on test set at epoch #84 is 0.8666666666666667\n",
            "Loss at epoch #84 is 0.33260451026647997\n",
            "Time at epoch #84 is 0.008616209030151367\n",
            "---------------FINISH epoch #84---------------\n",
            "---------------START epoch #85---------------\n",
            "weights [-0.02158446  0.17016468  0.02338987  0.33929747  0.17016468]\n",
            "bias [-0.15337586]\n",
            "Accuracy on test set at epoch #85 is 0.8666666666666667\n",
            "Loss at epoch #85 is 0.3312442191035698\n",
            "Time at epoch #85 is 0.022485017776489258\n",
            "---------------FINISH epoch #85---------------\n",
            "---------------START epoch #86---------------\n",
            "weights [-0.02211343  0.17188935  0.02271588  0.34140456  0.17188935]\n",
            "bias [-0.15506579]\n",
            "Accuracy on test set at epoch #86 is 0.87\n",
            "Loss at epoch #86 is 0.32989931936012723\n",
            "Time at epoch #86 is 0.020488262176513672\n",
            "---------------FINISH epoch #86---------------\n",
            "---------------START epoch #87---------------\n",
            "weights [-0.02263977  0.17360218  0.02204537  0.34350066  0.17360218]\n",
            "bias [-0.15675036]\n",
            "Accuracy on test set at epoch #87 is 0.87\n",
            "Loss at epoch #87 is 0.3285695811704672\n",
            "Time at epoch #87 is 0.021722078323364258\n",
            "---------------FINISH epoch #87---------------\n",
            "---------------START epoch #88---------------\n",
            "weights [-0.0231635   0.17530329  0.02137832  0.34558585  0.17530329]\n",
            "bias [-0.1584296]\n",
            "Accuracy on test set at epoch #88 is 0.8733333333333333\n",
            "Loss at epoch #88 is 0.32725477843699324\n",
            "Time at epoch #88 is 0.021874189376831055\n",
            "---------------FINISH epoch #88---------------\n",
            "---------------START epoch #89---------------\n",
            "weights [-0.02368462  0.17699276  0.0207147   0.34766022  0.17699276]\n",
            "bias [-0.16010354]\n",
            "Accuracy on test set at epoch #89 is 0.88\n",
            "Loss at epoch #89 is 0.32595468877405853\n",
            "Time at epoch #89 is 0.02210545539855957\n",
            "---------------FINISH epoch #89---------------\n",
            "---------------START epoch #90---------------\n",
            "weights [-0.02420318  0.17867069  0.0200545   0.34972384  0.17867069]\n",
            "bias [-0.16177223]\n",
            "Accuracy on test set at epoch #90 is 0.88\n",
            "Loss at epoch #90 is 0.3246690934519411\n",
            "Time at epoch #90 is 0.018862247467041016\n",
            "---------------FINISH epoch #90---------------\n",
            "---------------START epoch #91---------------\n",
            "weights [-0.02471917  0.18033717  0.01939769  0.35177679  0.18033717]\n",
            "bias [-0.1634357]\n",
            "Accuracy on test set at epoch #91 is 0.88\n",
            "Loss at epoch #91 is 0.3233977773410114\n",
            "Time at epoch #91 is 0.019532442092895508\n",
            "---------------FINISH epoch #91---------------\n",
            "---------------START epoch #92---------------\n",
            "weights [-0.02523263  0.1819923   0.01874424  0.35381916  0.1819923 ]\n",
            "bias [-0.16509398]\n",
            "Accuracy on test set at epoch #92 is 0.88\n",
            "Loss at epoch #92 is 0.3221405288561583\n",
            "Time at epoch #92 is 0.010152339935302734\n",
            "---------------FINISH epoch #92---------------\n",
            "---------------START epoch #93---------------\n",
            "weights [-0.02574356  0.18363617  0.01809413  0.35585101  0.18363617]\n",
            "bias [-0.16674712]\n",
            "Accuracy on test set at epoch #93 is 0.88\n",
            "Loss at epoch #93 is 0.3208971399015317\n",
            "Time at epoch #93 is 0.02258014678955078\n",
            "---------------FINISH epoch #93---------------\n",
            "---------------START epoch #94---------------\n",
            "weights [-0.02625199  0.18526888  0.01744734  0.35787243  0.18526888]\n",
            "bias [-0.16839514]\n",
            "Accuracy on test set at epoch #94 is 0.8833333333333333\n",
            "Loss at epoch #94 is 0.3196674058156526\n",
            "Time at epoch #94 is 0.020853281021118164\n",
            "---------------FINISH epoch #94---------------\n",
            "---------------START epoch #95---------------\n",
            "weights [-0.02675794  0.18689052  0.01680384  0.3598835   0.18689052]\n",
            "bias [-0.17003808]\n",
            "Accuracy on test set at epoch #95 is 0.8866666666666667\n",
            "Loss at epoch #95 is 0.31845112531693687\n",
            "Time at epoch #95 is 0.020480871200561523\n",
            "---------------FINISH epoch #95---------------\n",
            "---------------START epoch #96---------------\n",
            "weights [-0.02726142  0.18850117  0.01616362  0.36188428  0.18850117]\n",
            "bias [-0.17167597]\n",
            "Accuracy on test set at epoch #96 is 0.8866666666666667\n",
            "Loss at epoch #96 is 0.31724810044966856\n",
            "Time at epoch #96 is 0.02118539810180664\n",
            "---------------FINISH epoch #96---------------\n",
            "---------------START epoch #97---------------\n",
            "weights [-0.02776245  0.19010093  0.01552664  0.36387485  0.19010093]\n",
            "bias [-0.17330886]\n",
            "Accuracy on test set at epoch #97 is 0.8866666666666667\n",
            "Loss at epoch #97 is 0.3160581365304606\n",
            "Time at epoch #97 is 0.021757125854492188\n",
            "---------------FINISH epoch #97---------------\n",
            "---------------START epoch #98---------------\n",
            "weights [-0.02826106  0.19168989  0.01489288  0.36585528  0.19168989]\n",
            "bias [-0.17493677]\n",
            "Accuracy on test set at epoch #98 is 0.8866666666666667\n",
            "Loss at epoch #98 is 0.31488104209522816\n",
            "Time at epoch #98 is 0.017310380935668945\n",
            "---------------FINISH epoch #98---------------\n",
            "---------------START epoch #99---------------\n",
            "weights [-0.02875724  0.19326813  0.01426233  0.36782566  0.19326813]\n",
            "bias [-0.17655974]\n",
            "Accuracy on test set at epoch #99 is 0.89\n",
            "Loss at epoch #99 is 0.3137166288467038\n",
            "Time at epoch #99 is 0.019612550735473633\n",
            "---------------FINISH epoch #99---------------\n",
            "---------------START epoch #100---------------\n",
            "weights [-0.02925104  0.19483575  0.01363495  0.36978605  0.19483575]\n",
            "bias [-0.1781778]\n",
            "Accuracy on test set at epoch #100 is 0.8933333333333333\n",
            "Loss at epoch #100 is 0.312564711602514\n",
            "Time at epoch #100 is 0.022627592086791992\n",
            "---------------FINISH epoch #100---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvp1vXpNqA8D"
      },
      "source": [
        "nparraytocsv(w_save, \"weights_save-HRF_samples_small.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxsBj3VUf7Ce"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import threading\n",
        "\n",
        "class MTLR:\n",
        "\n",
        "    def __init__(self, threads, x_train, y_train, x_test, y_test, epoch, lr=0.01, context=None):\n",
        "        self._threads = threads\n",
        "        self._x_train = x_train\n",
        "        self._y_train = y_train\n",
        "        self._x_test = x_test\n",
        "        self._y_test = y_test\n",
        "        self._epoch = epoch\n",
        "        self._lr = lr\n",
        "        self._context = context\n",
        "\n",
        "        self._length = self._x_train.shape[0]\n",
        "        self._dim = self._x_train.shape[1]\n",
        "\n",
        "        self._delta_weights = np.zeros([self._threads, self._dim])\n",
        "        self._delta_bias = np.zeros(self._threads)\n",
        "        self._counts = np.zeros(self._threads)\n",
        "\n",
        "        self.weight = np.zeros(self._dim)\n",
        "        self.bias = np.zeros(1)\n",
        "\n",
        "        self._cut_points = np.append(np.array(range(self._threads))*int(self._length/self._threads), self._length)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self._epoch):\n",
        "            print(f\"---------------START epoch #{epoch + 1}---------------\")\n",
        "\n",
        "            t_start = time()\n",
        "\n",
        "            if self._context != None:\n",
        "                self.__encrypt()\n",
        "\n",
        "            running_threads = list()\n",
        "            for index in range(self._threads):\n",
        "                rt = threading.Thread(target=self.__thread_function, args=(index,))\n",
        "                running_threads.append(rt)\n",
        "                rt.start()\n",
        "\n",
        "            for each_thread in running_threads:\n",
        "                each_thread.join()\n",
        "\n",
        "            self.__update_parameters()\n",
        "\n",
        "            if self._context != None:\n",
        "                self.__decrypt()\n",
        "\n",
        "            t_end = time()\n",
        "\n",
        "            self.__reset()\n",
        "\n",
        "            ##acc = self.__get_test_accuracy(self._x_test, self._y_test)\n",
        "            ##print(f\"Accuracy on test set at epoch #{epoch + 1} is {acc}\")\n",
        "            print(f\"Time at epoch #{epoch + 1} is {t_end - t_start}\")\n",
        "            print(f\"---------------FINISH epoch #{epoch + 1}---------------\")\n",
        "\n",
        "\n",
        "    def __thread_function(self, index):\n",
        "        for x, y in zip(self._x_train[index:index+1,:], self._y_train[index:index+1]):\n",
        "            out = self.__approximate_sigmoid(x.dot(self.weight) + self.bias)\n",
        "            out_minus_y = out - y\n",
        "            self._delta_weights[index,:] += x * out_minus_y\n",
        "            self._delta_bias[index] += out_minus_y\n",
        "            self._counts[index] += 1\n",
        "\n",
        "    def __approximate_sigmoid(self, x):\n",
        "        # We use the polynomial approximation of degree 3\n",
        "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "        # from https://eprint.iacr.org/2018/462.pdf\n",
        "        # which fits the function pretty well in the range [-5,5]\n",
        "        return 0.5 + 0.197 * x - 0.004 * x * x * x\n",
        "\n",
        "    def __encrypt(self):\n",
        "        self.weight = ts.ckks_vector(self._context, self.weight)\n",
        "        self.bias = ts.ckks_vector(self._context, self.bias)\n",
        "\n",
        "    def __decrypt(self):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "    def __update_parameters(self):\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._lr * np.sum(self._delta_weights / np.vstack(self._counts), axis=0)\n",
        "        self.bias -= self._lr * np.sum(self._delta_bias / np.vstack(self._counts))\n",
        "\n",
        "    def __reset(self):\n",
        "        self._delta_weights = np.zeros([self._threads, self._dim])\n",
        "        self._delta_bias = np.zeros(self._threads)\n",
        "        self._counts = np.zeros(self._threads)\n",
        "\n",
        "    def __get_test_accuracy(self, X_test, Y_test):\n",
        "        print(\"weights\", self.weight)\n",
        "        print(\"bias\", self.bias)\n",
        "        clf_acc = LogisticRegression()\n",
        "        clf_acc.classes_ = np.unique(Y_test)\n",
        "        clf_acc.coef_ = self.weight\n",
        "        clf_acc.intercept_ = self.bias\n",
        "        #w = clf_acc.coef_[0]\n",
        "        #a = -w[0] / w[1]\n",
        "        #xx = np.linspace(-5, 5)\n",
        "        #yy = a * xx - (clf_acc.intercept_[0]) / w[1]\n",
        "\n",
        "        #plt.plot(xx, yy, 'k-')\n",
        "        #plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
        "        #plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")\n",
        "        #plt.show()\n",
        "        return clf_acc.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_awNNyPit1b"
      },
      "source": [
        "X, y = csvtonparray(\"LogReg_sample_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A0RWqKQh5Ie",
        "outputId": "8173496f-08f8-4296-b40e-78d1f82c3a1b"
      },
      "source": [
        "X_train, y_train, X_test, y_test = split_train_test(X, y)\n",
        "t_start = time()\n",
        "enc_X_train, enc_y_train = encrypt_train_data(X_train, y_train)\n",
        "print(f\"Encryption runtime is {time() - t_start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encryption runtime is 45.031067848205566\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}